{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, model_selection\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn.utils import prune\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Running on device:\", DEVICE.upper())\n",
    "\n",
    "# manual random seed is used for dataset partitioning\n",
    "# to ensure reproducible results across runs\n",
    "SEED = 42\n",
    "RNG = torch.Generator().manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "# Create an unverified SSL context\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and pre-process CIFAR10\n",
    "normalize = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(\n",
    "    root=\"../example notebooks/data\", train=True, download=False, transform=normalize\n",
    ")\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "# we split held out data into test and validation set\n",
    "held_out = torchvision.datasets.CIFAR10(\n",
    "    root=\"../example notebooks/data\", train=False, download=False, transform=normalize\n",
    ")\n",
    "test_set, val_set = torch.utils.data.random_split(held_out, [0.5, 0.5], generator=RNG)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "# download the forget and retain index split\n",
    "local_path = \"../example notebooks/forget_idx.npy\"\n",
    "# if not os.path.exists(local_path):\n",
    "#     response = requests.get(\n",
    "#         \"https://storage.googleapis.com/unlearning-challenge/\" + local_path\n",
    "#     )\n",
    "#     open(local_path, \"wb\").write(response.content)\n",
    "forget_idx = np.load(local_path)\n",
    "\n",
    "# construct indices of retain from those of the forget set\n",
    "forget_mask = np.zeros(len(train_set.targets), dtype=bool)\n",
    "forget_mask[forget_idx] = True\n",
    "retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
    "\n",
    "# split train set into a forget and a retain set\n",
    "forget_set = torch.utils.data.Subset(train_set, forget_idx)\n",
    "retain_set = torch.utils.data.Subset(train_set, retain_idx)\n",
    "\n",
    "forget_loader = torch.utils.data.DataLoader(\n",
    "    forget_set, batch_size=batch_size, shuffle=True, num_workers=1\n",
    ")\n",
    "# retain_loader = torch.utils.data.DataLoader(\n",
    "#     retain_set, batch_size=128, shuffle=True, num_workers=1, generator=RNG\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retain_loader = torch.utils.data.DataLoader(\n",
    "    retain_set, batch_size=batch_size, shuffle=True, num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Retain set accuracy: 99.5%\n",
    "- Forget set accuracy: 99.3%\n",
    "- Val set accuracy: 88.9%\n",
    "- Test set accuracy: 88.3%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"../example notebooks/weights/weights_resnet18_cifar10.pth\"\n",
    "if not os.path.exists(local_path):\n",
    "    response = requests.get(\n",
    "        \"https://storage.googleapis.com/unlearning-challenge/weights_resnet18_cifar10.pth\"\n",
    "    )\n",
    "    open(local_path, \"wb\").write(response.content)\n",
    "\n",
    "weights_pretrained = torch.load(local_path, map_location=DEVICE) #43Mbs\n",
    "\n",
    "# load net with pre-trained weights\n",
    "net = resnet18(weights=None, num_classes=10)\n",
    "net.load_state_dict(weights_pretrained)\n",
    "net.to(DEVICE)\n",
    "net.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_losses = compute_losses(net, val_loader)\n",
    "# test_losses = compute_losses(net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pruning\n",
    "# pct = 0.10\n",
    "# unstructure_prune(net, pct, global_pruning=True, random_init=False)\n",
    "\n",
    "torch.save({\n",
    "    'net': net.state_dict(),\n",
    "}, f'./checkpoints/temp_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forget_step(net, forget_loader, starting_forget_acc, counter_bool=0):\n",
    "\n",
    "    # Initialize for all epochs\n",
    "    forget_acc = copy.copy(starting_forget_acc)\n",
    "    print(f'Starting with {100.0 * forget_acc:0.2f}% forget accuracy')\n",
    "    forget_acc_threshold_1 = forget_acc*0.90\n",
    "    forget_acc_threshold_2 = forget_acc*0.80\n",
    "    print(f'Thresholds: {100.0 * forget_acc_threshold_1:0.2f}%, {100.0 * forget_acc_threshold_2:0.2f}%')\n",
    "\n",
    "    \n",
    "    iter_forget = iter(forget_loader)\n",
    "    current_batch = 0\n",
    "\n",
    "    initial_forget_lr = LR\n",
    "    if counter_bool>0:\n",
    "        initial_forget_lr = initial_forget_lr*(1.5**counter_bool)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.0)\n",
    "    forget_optimizer = optim.AdamW(net.parameters(), lr=initial_forget_lr)\n",
    "\n",
    "    not_depleted = True\n",
    "    counter = 0\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    while (forget_acc > forget_acc_threshold_1) & (not_depleted):\n",
    "\n",
    "        try:\n",
    "            sample = next(iter_forget)\n",
    "            counter+=1\n",
    "            \n",
    "        except StopIteration:\n",
    "            not_depleted = False\n",
    "            print('depleted')\n",
    "            break\n",
    "\n",
    "        inputs, targets = sample\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        forget_optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = net(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        classification_loss = criterion(logits, targets)\n",
    "\n",
    "        loss = -1*classification_loss\n",
    "        loss.backward()\n",
    "        forget_optimizer.step()\n",
    "\n",
    "        current_batch+=1\n",
    "        print(current_batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            forget_acc = accuracy(net, forget_loader)\n",
    "        print(f\"Forget set accuracy: {100.0 * forget_acc:0.2f}%\")\n",
    "        print('--'*10)\n",
    "\n",
    "        if (forget_acc<forget_acc_threshold_2):\n",
    "            initial_forget_lr = initial_forget_lr/2\n",
    "            current_batch = 0\n",
    "            forget_acc = starting_forget_acc\n",
    "            print('Restoring')\n",
    "            checkpoint = torch.load(f'./checkpoints/temp_checkpoint.pth')\n",
    "            net.load_state_dict(checkpoint['net'])\n",
    "            forget_optimizer = optim.AdamW(net.parameters(), lr=initial_forget_lr)\n",
    "\n",
    "        if counter>=10:\n",
    "            break\n",
    "\n",
    "    return net, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_step(net, retain_loader):\n",
    "\n",
    "    initial_retain_lr = LR\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_retain = optim.SGD(net.parameters(), lr=initial_retain_lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    warmup_current_batch = 0\n",
    "    warmup_batches = math.ceil(0.4*len(retain_loader.dataset))\n",
    "    \n",
    "    net.train()\n",
    "\n",
    "    for sample in retain_loader:\n",
    "\n",
    "        inputs, targets = sample\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        warmup_current_batch += 1\n",
    "\n",
    "        # Warm-up for the first 'warmup_batches' batches\n",
    "        if warmup_current_batch <= warmup_batches:\n",
    "            adjust_learning_rate(optimizer_retain, warmup_current_batch, warmup_batches, initial_retain_lr)\n",
    "\n",
    "        optimizer_retain.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = net(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.4)\n",
    "        classification_loss = criterion(logits, targets)\n",
    "        loss = classification_loss\n",
    "        loss.backward()\n",
    "        optimizer_retain.step()\n",
    "\n",
    "    torch.save({\n",
    "        'net': net.state_dict(),\n",
    "    }, f'./checkpoints/temp_checkpoint.pth')\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "counter_bool = 0\n",
    "\n",
    "for ep in range(epochs):\n",
    "\n",
    "    print(f'Epoch: {ep}')\n",
    "    print('****'*10)\n",
    "\n",
    "    if ep!=epochs-1:\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            starting_forget_acc = accuracy(net, forget_loader)\n",
    "\n",
    "        if starting_forget_acc > 0.86:\n",
    "            net, counter = forget_step(net, forget_loader, starting_forget_acc, counter_bool)\n",
    "            if counter>=10:\n",
    "                counter_bool+=1\n",
    "                print(f'COUNTER BOOL ON = {counter_bool}')\n",
    "        else:\n",
    "            print('Not doing forget step')\n",
    "\n",
    "    print('\"\"\" \"\"\"'*5)\n",
    "    print('Retrain')\n",
    "    net = retrain_step(net, retain_loader)\n",
    "\n",
    "    ft_forget_losses = compute_losses(net, forget_loader)\n",
    "\n",
    "    ft_mia_scores = calc_mia_acc(ft_forget_losses, val_losses)\n",
    "\n",
    "    print(\n",
    "        f\"The MIA has an accuracy of {ft_mia_scores.mean():.3f} on forgotten vs unseen images\"\n",
    "    )\n",
    "\n",
    "    if np.abs(0.5-ft_mia_scores.mean())<0.01:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.Figure(figsize=(16, 6))\n",
    "\n",
    "plt.title(\n",
    "    f\"Unlearned by fine-tuning.\\nAttack accuracy: {ft_mia_scores.mean():0.2f}\"\n",
    ")\n",
    "plt.hist(val_losses, density=True, alpha=0.5, bins=50, label=\"Test set\")\n",
    "plt.hist(ft_forget_losses, density=True, alpha=0.5, bins=50, label=\"Forget set\")\n",
    "\n",
    "plt.xlabel(\"Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlim((0, np.max(val_losses)))\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
