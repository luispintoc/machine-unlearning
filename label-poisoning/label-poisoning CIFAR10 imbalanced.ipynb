{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6YXq1oMM3bj",
    "outputId": "eb1d1857-20e9-4bf1-e849-87d953ed74ac"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Running on device:\", DEVICE.upper())\n",
    "\n",
    "# manual random seed is used for dataset partitioning\n",
    "# to ensure reproducible results across runs\n",
    "RNG = torch.Generator().manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CF7zXiGLM3bk"
   },
   "source": [
    "# üíæ Download dataset and pre-trained model\n",
    "\n",
    "In this section, we'll load a sample dataset (CIFAR-10), a pre-trained model (ResNet18) trained on CIFAR-10, plot some images and compute the accuracy of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../imbalanced_data/imbalanced_dataset.pkl', 'rb') as file:\n",
    "    imbalanced_aug_dataset_id = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_list, transform=None):\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y, z = self.data_list[idx]\n",
    "        return (x, y)  # Explicitly return as tuple\n",
    "\n",
    "# download and pre-process CIFAR10\n",
    "normalize = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = CustomDataset(train_set, transform=normalize)\n",
    "train_dataset = CustomDataset(val_set, transform=normalize)\n",
    "train_dataset = CustomDataset(test_set, transform=normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a temporary data loader without normalization, just to show the images\n",
    "tmp_dl = DataLoader(train_dataset,\n",
    "    batch_size=16 * 5,\n",
    "    shuffle=False,\n",
    ")\n",
    "images, labels = next(iter(tmp_dl))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.title(\"Sample images from CIFAR10 dataset\")\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g5G7DBCoM3bl",
    "outputId": "9c9ad7ff-a02e-45b1-e3d9-e1f41843a5cb"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(\n",
    "    root=\"../example notebooks/data\", train=True, download=False, transform=normalize\n",
    ")\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "# we split held out data into test and validation set\n",
    "held_out = torchvision.datasets.CIFAR10(\n",
    "    root=\"../example notebooks/data\", train=False, download=False, transform=normalize\n",
    ")\n",
    "test_set, val_set = torch.utils.data.random_split(held_out, [0.5, 0.5], generator=RNG)\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
    "val_loader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "# download the forget and retain index split\n",
    "local_path = \"../example notebooks/forget_idx.npy\"\n",
    "# if not os.path.exists(local_path):\n",
    "#     response = requests.get(\n",
    "#         \"https://storage.googleapis.com/unlearning-challenge/\" + local_path\n",
    "#     )\n",
    "#     open(local_path, \"wb\").write(response.content)\n",
    "forget_idx = np.load(local_path)\n",
    "\n",
    "# construct indices of retain from those of the forget set\n",
    "forget_mask = np.zeros(len(train_set.targets), dtype=bool)\n",
    "forget_mask[forget_idx] = True\n",
    "retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
    "\n",
    "# split train set into a forget and a retain set\n",
    "forget_set = torch.utils.data.Subset(train_set, forget_idx)\n",
    "retain_set = torch.utils.data.Subset(train_set, retain_idx)\n",
    "\n",
    "forget_loader = torch.utils.data.DataLoader(\n",
    "    forget_set, batch_size=128, shuffle=True, num_workers=2\n",
    ")\n",
    "retain_loader = torch.utils.data.DataLoader(\n",
    "    retain_set, batch_size=128, shuffle=True, num_workers=2, generator=RNG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiwASOO1wzsx"
   },
   "source": [
    "We'll now download the weights of the model trained in CIFAR-10 and load them in a Pytorch model. This model has been trained using SGD with a learning rate of 0.1, momentum of 0.9 and weight decay of 5e-4. It was also trained using data augmentation. In particular, the transforms used to the data were:\n",
    "\n",
    "```python\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Rn6UKX_M3bl"
   },
   "outputs": [],
   "source": [
    "# load pre-trained weights\n",
    "local_path = \"../example notebooks/weights/weights_resnet18_cifar10.pth\"\n",
    "if not os.path.exists(local_path):\n",
    "    response = requests.get(\n",
    "        \"https://storage.googleapis.com/unlearning-challenge/weights_resnet18_cifar10.pth\"\n",
    "    )\n",
    "    open(local_path, \"wb\").write(response.content)\n",
    "\n",
    "weights_pretrained = torch.load(local_path, map_location=DEVICE) #43Mbs\n",
    "\n",
    "# load model with pre-trained weights\n",
    "model = resnet18(weights=None, num_classes=10)\n",
    "model.load_state_dict(weights_pretrained)\n",
    "model.to(DEVICE)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OhF8zu1M3bl"
   },
   "source": [
    "Let us show some of the training images, just for fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "id": "ASG-8jPEM3bl",
    "outputId": "61bd1212-c845-460f-9c49-1a2745a05da3"
   },
   "outputs": [],
   "source": [
    "# a temporary data loader without normalization, just to show the images\n",
    "tmp_dl = DataLoader(\n",
    "    torchvision.datasets.CIFAR10(\n",
    "        root=\"../example notebooks/data\", train=True, download=False, transform=transforms.ToTensor()\n",
    "    ),\n",
    "    batch_size=16 * 5,\n",
    "    shuffle=False,\n",
    ")\n",
    "images, labels = next(iter(tmp_dl))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.title(\"Sample images from CIFAR10 dataset\")\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZWK7sDSatvj"
   },
   "source": [
    "We'll now compute the model's accuracy on the train and test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X2hegPagM3bl",
    "outputId": "2678bf07-3af5-4029-81eb-339e7c1f9436"
   },
   "outputs": [],
   "source": [
    "def accuracy(net, loader):\n",
    "    \"\"\"Return accuracy on a dataset given by the data loader.\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "print(f\"Train set accuracy: {100.0 * accuracy(model, train_loader):0.1f}%\")\n",
    "print(f\"Test set accuracy: {100.0 * accuracy(model, test_loader):0.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXEWkDTTn4iO"
   },
   "source": [
    "# üéØ Unlearning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = resnet18(weights=None, num_classes=10)\n",
    "ft_model.load_state_dict(weights_pretrained)\n",
    "ft_model.to(DEVICE)\n",
    "\n",
    "ft_model.eval()\n",
    "preds = ft_model(x.to(DEVICE))\n",
    "_, targets = preds.max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvLkBLisM3bm"
   },
   "outputs": [],
   "source": [
    "def unlearning(net, retain, forget, validation):\n",
    "    \"\"\"Unlearning by fine-tuning.\n",
    "\n",
    "    Fine-tuning is a very simple algorithm that trains using only\n",
    "    the retain set.\n",
    "\n",
    "    Args:\n",
    "      net : nn.Module.\n",
    "        pre-trained model to use as base of unlearning.\n",
    "      retain : torch.utils.data.DataLoader.\n",
    "        Dataset loader for access to the retain set. This is the subset\n",
    "        of the training set that we don't want to forget.\n",
    "      forget : torch.utils.data.DataLoader.\n",
    "        Dataset loader for access to the forget set. This is the subset\n",
    "        of the training set that we want to forget. This method doesn't\n",
    "        make use of the forget set.\n",
    "      validation : torch.utils.data.DataLoader.\n",
    "        Dataset loader for access to the validation set. This method doesn't\n",
    "        make use of the validation set.\n",
    "    Returns:\n",
    "      net : updated model\n",
    "    \"\"\"\n",
    "    epochs = 1\n",
    "    early_stop_steps = 10  # Number of steps to consider for early stopping\n",
    "    early_stop_threshold = 0.03  # 5% validation performance decrease\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9, weight_decay=0)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    # Record the original performance on the validation set\n",
    "    original_val_accuracy = accuracy(net, validation)\n",
    "    print(f'Original val acc: {original_val_accuracy}')\n",
    "    \n",
    "    net.train()\n",
    "    steps_without_improvement = 0\n",
    "    step_count = 0\n",
    "\n",
    "    # combined_loader = combine_loaders_randomly(retain, forget, net)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "\n",
    "        # Initialize iterators at the beginning of each epoch\n",
    "        iter_retain = iter(retain)\n",
    "        iter_forget = iter(forget)\n",
    "\n",
    "        # Initialize weights for random selection\n",
    "        weights = torch.tensor([90, 10], dtype=torch.float32)\n",
    "\n",
    "        # Initialize flag for depleted iterator\n",
    "        one_depleted = False\n",
    "\n",
    "        # while not one_depleted:\n",
    "        for _ in range(150):\n",
    "          # Randomly choose a loader based on weights\n",
    "          chosen_loader_idx = torch.multinomial(weights, 1).item()\n",
    "          \n",
    "          if chosen_loader_idx == 0:  # corresponds to retain_loader\n",
    "              try:\n",
    "                  batch = next(iter_retain)\n",
    "              except StopIteration:\n",
    "                  one_depleted = True\n",
    "                  continue  # Skip to the next iteration\n",
    "                  \n",
    "          else:  # corresponds to forget_loader\n",
    "              try:\n",
    "                  batch = next(iter_forget)\n",
    "              except StopIteration:\n",
    "                  one_depleted = True\n",
    "                  continue  # Skip to the next iteration\n",
    "          # print(batch)\n",
    "          # for inputs, targets in batch:\n",
    "          inputs, targets = batch\n",
    "\n",
    "          if chosen_loader_idx == 1:\n",
    "              print('aa')\n",
    "              # Compute new targets\n",
    "              with torch.no_grad():\n",
    "                  net.eval()\n",
    "                  preds = net(inputs.to(DEVICE))\n",
    "                  _, targets = torch.topk(preds, 2, dim=1)\n",
    "                  targets = targets[:, -1]\n",
    "          inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "          net.train()\n",
    "          optimizer.zero_grad()\n",
    "          outputs = net(inputs)\n",
    "          loss = criterion(outputs, targets)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          step_count += 1\n",
    "\n",
    "          # Evaluate on validation set\n",
    "          current_val_accuracy = accuracy(net, validation)\n",
    "          print(f\"Validation acc {current_val_accuracy:.4f}\")\n",
    "\n",
    "          # Check for performance decrease\n",
    "          if current_val_accuracy < (1 - early_stop_threshold) * original_val_accuracy:\n",
    "              steps_without_improvement += 1\n",
    "              if steps_without_improvement >= early_stop_steps:\n",
    "                  print(f\"Early stopping triggered at step {step_count}.\")\n",
    "                  net.eval()\n",
    "                  return net\n",
    "          else:\n",
    "              steps_without_improvement = 0\n",
    "\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    net.eval()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o27hc1HOM3bm"
   },
   "outputs": [],
   "source": [
    "ft_model = resnet18(weights=None, num_classes=10)\n",
    "ft_model.load_state_dict(weights_pretrained)\n",
    "ft_model.to(DEVICE)\n",
    "\n",
    "# Execute the unlearing routine. This might take a few minutes.\n",
    "# If run on colab, be sure to be running it on  an instance with GPUs\n",
    "ft_model = unlearning(ft_model, retain_loader, forget_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1--PInKr9-6B"
   },
   "source": [
    "We have now an unlearned model `ft_model`. Besides the forgetting quality (which we'll discuss in the next section), a good unlearning algorithm should retain as much as possible the accuracy on the retain and test set.\n",
    "\n",
    "To quantify this potential loss of utility, we'll now compute the retain and test accuracies using the unlearned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wqp7rN2E9-6B",
    "outputId": "8f328932-a5ca-4d92-b83a-8332f430a12b"
   },
   "outputs": [],
   "source": [
    "print(f\"Retain set accuracy: {100.0 * accuracy(ft_model, retain_loader):0.1f}%\")\n",
    "print(f\"Test set accuracy: {100.0 * accuracy(ft_model, test_loader):0.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xccyUQcuM3bl"
   },
   "source": [
    "# üèÖ Evaluation\n",
    "\n",
    "In this section, we'll quantify the quality of the unlearning algorithm through a simple membership inference attack (MIA). We provide this simple MIA for convenience so that participants can quickly obtain a metric for their unlearning algorithm, but submissions will be scored using a different method.\n",
    "\n",
    "This MIA consists of a [logistic regression model](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) that predicts whether the model was trained on a particular sample from that sample's loss. To get an idea on the difficulty of this problem, we first plot below a histogram of the losses of the pre-trained model on the train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWUxHD3EM3bm"
   },
   "outputs": [],
   "source": [
    "def compute_losses(net, loader):\n",
    "    \"\"\"Auxiliary function to compute per-sample losses\"\"\"\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    all_losses = []\n",
    "\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        logits = net(inputs)\n",
    "        losses = criterion(logits, targets).numpy(force=True)\n",
    "        for l in losses:\n",
    "            all_losses.append(l)\n",
    "\n",
    "    return np.array(all_losses)\n",
    "\n",
    "\n",
    "train_losses = compute_losses(model, train_loader)\n",
    "test_losses = compute_losses(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "id": "j2t4sYtZM3bm",
    "outputId": "582a3a8e-f2ee-42c3-a60d-70e27e4af104"
   },
   "outputs": [],
   "source": [
    "# plot losses on train and test set\n",
    "plt.title(\"Losses on train and test set (pre-trained model)\")\n",
    "plt.hist(test_losses, density=True, alpha=0.5, bins=50, label=\"Test set\")\n",
    "plt.hist(train_losses, density=True, alpha=0.5, bins=50, label=\"Train set\")\n",
    "plt.xlabel(\"Loss\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "plt.xlim((0, np.max(test_losses)))\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(frameon=False, fontsize=14)\n",
    "ax = plt.gca()\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApxbefWMM3bm"
   },
   "source": [
    "As per the above plot, the distributions of losses are quite different between the train and test sets, as expected. In what follows, we will define an MIA that leverages the fact that examples that were trained on have smaller losses compared to examples that weren't. Using this fact, the simple MIA defined below will aim to infer whether the forget set was in fact part of the training set.\n",
    "\n",
    "This MIA is defined below. It takes as input the per-sample losses of the unlearned model on forget and test examples, and a membership label (0 or 1) indicating which of those two groups each sample comes from. It then returns the cross-validation accuracy of a linear model trained to distinguish between the two classes.\n",
    "\n",
    "Intuitively, an unlearning algorithm is successful with respect to this simple metric if the attacker isn't able to distinguish the forget set from the test set any better than it would for the ideal unlearning algorithm (retraining from scratch without the retain set); see the last section of this notebook for additional discussion and for computing that reference point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6gBtdz7AM3bm"
   },
   "outputs": [],
   "source": [
    "def simple_mia(sample_loss, members, n_splits=10, random_state=0):\n",
    "    \"\"\"Computes cross-validation score of a membership inference attack.\n",
    "\n",
    "    Args:\n",
    "      sample_loss : array_like of shape (n,).\n",
    "        objective function evaluated on n samples.\n",
    "      members : array_like of shape (n,),\n",
    "        whether a sample was used for training.\n",
    "      n_splits: int\n",
    "        number of splits to use in the cross-validation.\n",
    "    Returns:\n",
    "      scores : array_like of size (n_splits,)\n",
    "    \"\"\"\n",
    "\n",
    "    unique_members = np.unique(members)\n",
    "    if not np.all(unique_members == np.array([0, 1])):\n",
    "        raise ValueError(\"members should only have 0 and 1s\")\n",
    "\n",
    "    attack_model = linear_model.LogisticRegression()\n",
    "    cv = model_selection.StratifiedShuffleSplit(\n",
    "        n_splits=n_splits, random_state=random_state\n",
    "    )\n",
    "    return model_selection.cross_val_score(\n",
    "        attack_model, sample_loss, members, cv=cv, scoring=\"accuracy\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0uxLiyO9-6C"
   },
   "source": [
    "As a reference point, we first compute the accuracy of the MIA on the original model to distinguish between the forget set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ekuEyHlUM3bm",
    "outputId": "b4429f14-7cf4-4f09-9798-e9b4c6662dad"
   },
   "outputs": [],
   "source": [
    "forget_losses = compute_losses(model, forget_loader)\n",
    "\n",
    "# Since we have more forget losses than test losses, sub-sample them, to have a class-balanced dataset.\n",
    "np.random.shuffle(forget_losses)\n",
    "forget_losses = forget_losses[: len(test_losses)]\n",
    "\n",
    "samples_mia = np.concatenate((test_losses, forget_losses)).reshape((-1, 1))\n",
    "labels_mia = [0] * len(test_losses) + [1] * len(forget_losses)\n",
    "\n",
    "mia_scores = simple_mia(samples_mia, labels_mia)\n",
    "\n",
    "print(\n",
    "    f\"The MIA has an accuracy of {mia_scores.mean():.3f} on forgotten vs unseen images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12Ek9x4zM3bm"
   },
   "source": [
    "We'll now compute the accuracy of the MIA on the unlearned model. We expect the MIA to be less accurate on the unlearned model than on the original model, since the original model has not undergone a procedure to unlearn the forget set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYgYXRvPM3bm"
   },
   "outputs": [],
   "source": [
    "ft_forget_losses = compute_losses(ft_model, forget_loader)\n",
    "ft_test_losses = compute_losses(ft_model, test_loader)\n",
    "\n",
    "# make sure we have a balanced dataset for the MIA\n",
    "assert len(ft_test_losses) == len(ft_forget_losses)\n",
    "\n",
    "ft_samples_mia = np.concatenate((ft_test_losses, ft_forget_losses)).reshape((-1, 1))\n",
    "labels_mia = [0] * len(ft_test_losses) + [1] * len(ft_forget_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BWvzT3cPGvYs",
    "outputId": "f93e24d2-de6d-41d1-df70-e6625be74898"
   },
   "outputs": [],
   "source": [
    "ft_mia_scores = simple_mia(ft_samples_mia, labels_mia)\n",
    "\n",
    "print(\n",
    "    f\"The MIA has an accuracy of {ft_mia_scores.mean():.3f} on forgotten vs unseen images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyTh5ej49-6C"
   },
   "source": [
    "From the score above, the MIA is indeed less accurate on the unlearned model than on the original model, as expected. Finally, we'll plot the histogram of losses of the unlearned model on the train and test set. From the below figure, we can observe that the distributions of forget and test losses are more similar under the unlearned model compared to the original model, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "id": "bJ8JI-GDM3bn",
    "outputId": "f952d6a8-f39d-4297-d60f-3db076c03e1b"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax1.set_title(f\"Pre-trained model.\\nAttack accuracy: {mia_scores.mean():0.2f}\")\n",
    "ax1.hist(test_losses, density=True, alpha=0.5, bins=50, label=\"Test set\")\n",
    "ax1.hist(forget_losses, density=True, alpha=0.5, bins=50, label=\"Forget set\")\n",
    "\n",
    "ax2.set_title(\n",
    "    f\"Unlearned by fine-tuning.\\nAttack accuracy: {ft_mia_scores.mean():0.2f}\"\n",
    ")\n",
    "ax2.hist(ft_test_losses, density=True, alpha=0.5, bins=50, label=\"Test set\")\n",
    "ax2.hist(ft_forget_losses, density=True, alpha=0.5, bins=50, label=\"Forget set\")\n",
    "\n",
    "ax1.set_xlabel(\"Loss\")\n",
    "ax2.set_xlabel(\"Loss\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.set_yscale(\"log\")\n",
    "ax2.set_yscale(\"log\")\n",
    "ax1.set_xlim((0, np.max(test_losses)))\n",
    "ax2.set_xlim((0, np.max(test_losses)))\n",
    "for ax in (ax1, ax2):\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "ax1.legend(frameon=False, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqeUwUVO-NjD"
   },
   "source": [
    "## Comparison with a re-trained model\n",
    "\n",
    "One might ask, how good are the scores above? What is the best score? Since our goal is to approximate the model that has been trained only on the retain set, we'll consider that the gold standard is the score achieved by this model. Intuitively, we expect the MIA accuracy to be around 0.5, since for such a model, both the forget and test set are unseen samples from the same distribution. However, a number of factors such as distribution shift or class imbalance can make this number vary.\n",
    "\n",
    "We'll now compute this score. We'll first download the weights for a model trained exclusively on the retain set and then compute the accuracy of the simple MIA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44XEVh4-_Fz5",
    "outputId": "97a66783-755d-40e2-8098-009f61195293"
   },
   "outputs": [],
   "source": [
    "# download weights of a model trained exclusively on the retain set\n",
    "local_path = \"weights/retrain_weights_resnet18_cifar10.pth\"\n",
    "if not os.path.exists(local_path):\n",
    "    response = requests.get(\n",
    "        \"https://storage.googleapis.com/unlearning-challenge/retrain_weights_resnet18_cifar10.pth\"\n",
    "    )\n",
    "    open(local_path, \"wb\").write(response.content)\n",
    "\n",
    "weights_pretrained = torch.load(local_path, map_location=DEVICE)\n",
    "\n",
    "# load model with pre-trained weights\n",
    "rt_model = resnet18(weights=None, num_classes=10)\n",
    "rt_model.load_state_dict(weights_pretrained)\n",
    "rt_model.to(DEVICE)\n",
    "rt_model.eval()\n",
    "\n",
    "# print its accuracy on retain and forget set\n",
    "print(f\"Retain set accuracy: {100.0 * accuracy(rt_model, retain_loader):0.1f}%\")\n",
    "print(f\"Forget set accuracy: {100.0 * accuracy(rt_model, forget_loader):0.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnqSz58yG92l"
   },
   "source": [
    "As expected, the model trained exclusively on the retain set has a higher accuracy on the retain set than on the forget set (whose accuracy is similar than on the test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PqGSYyuE_GtI"
   },
   "outputs": [],
   "source": [
    "rt_test_losses = compute_losses(rt_model, test_loader)\n",
    "rt_forget_losses = compute_losses(rt_model, forget_loader)\n",
    "\n",
    "rt_samples_mia = np.concatenate((rt_test_losses, rt_forget_losses)).reshape((-1, 1))\n",
    "labels_mia = [0] * len(rt_test_losses) + [1] * len(rt_forget_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YLedO-Ps_q8n",
    "outputId": "b45a9d12-b909-4783-aa2c-ebd12c01831b"
   },
   "outputs": [],
   "source": [
    "rt_mia_scores = simple_mia(rt_samples_mia, labels_mia)\n",
    "\n",
    "print(\n",
    "    f\"The MIA has an accuracy of {rt_mia_scores.mean():.3f} on forgotten vs unseen images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9vbFFPZDP-W"
   },
   "source": [
    "As we expect, the accuracy of the MIA attack is roughly 0.5. Finally, as we've done before, let's compare the histograms of this ideal algorithm (re-trained model) vs the model obtain from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "id": "okhS6CSOAT1N",
    "outputId": "bc278e8e-93af-4b57-db84-6b0f908592d6"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax1.set_title(f\"Re-trained model.\\nAttack accuracy: {rt_mia_scores.mean():0.2f}\")\n",
    "ax1.hist(rt_test_losses, density=True, alpha=0.5, bins=50, label=\"Test set\")\n",
    "ax1.hist(rt_forget_losses, density=True, alpha=0.5, bins=50, label=\"Forget set\")\n",
    "\n",
    "ax2.set_title(\n",
    "    f\"Unlearned by fine-tuning.\\nAttack accuracy: {ft_mia_scores.mean():0.2f}\"\n",
    ")\n",
    "ax2.hist(ft_test_losses, density=True, alpha=0.5, bins=50, label=\"Test set\")\n",
    "ax2.hist(ft_forget_losses, density=True, alpha=0.5, bins=50, label=\"Forget set\")\n",
    "\n",
    "ax1.set_xlabel(\"Loss\")\n",
    "ax2.set_xlabel(\"Loss\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.set_yscale(\"log\")\n",
    "ax2.set_yscale(\"log\")\n",
    "ax1.set_xlim((0, np.max(test_losses)))\n",
    "ax2.set_xlim((0, np.max(test_losses)))\n",
    "for ax in (ax1, ax2):\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "ax1.legend(frameon=False, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYTGGV56EEMN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
