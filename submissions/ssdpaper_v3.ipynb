{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import subprocess\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision.models import resnet18\n","from torch.utils.data import DataLoader, Dataset, ConcatDataset, dataset, Subset\n","\n","from typing import Dict, List\n","\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T21:43:34.162749Z","iopub.status.busy":"2023-09-25T21:43:34.162183Z","iopub.status.idle":"2023-09-25T21:43:34.168488Z","shell.execute_reply":"2023-09-25T21:43:34.167420Z","shell.execute_reply.started":"2023-09-25T21:43:34.162713Z"},"trusted":true},"outputs":[],"source":["# It's really important to add an accelerator to your notebook, as otherwise the submission will fail.\n","# We recomment using the P100 GPU rather than T4 as it's faster and will increase the chances of passing the time cut-off threshold.\n","\n","if DEVICE != 'cuda':\n","    raise RuntimeError('Make sure you have added an accelerator to your notebook; the submission will fail otherwise!')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T21:43:34.170796Z","iopub.status.busy":"2023-09-25T21:43:34.170101Z","iopub.status.idle":"2023-09-25T21:43:34.184486Z","shell.execute_reply":"2023-09-25T21:43:34.183448Z","shell.execute_reply.started":"2023-09-25T21:43:34.170757Z"},"trusted":true},"outputs":[],"source":["# Helper functions for loading the hidden dataset.\n","\n","def load_example(df_row):\n","    image = torchvision.io.read_image(df_row['image_path'])\n","    result = {\n","        'image': image,\n","        'image_id': df_row['image_id'],\n","        'age_group': df_row['age_group'],\n","        'age': df_row['age'],\n","        'person_id': df_row['person_id']\n","    }\n","    return result\n","\n","\n","class HiddenDataset(Dataset):\n","    '''The hidden dataset.'''\n","    def __init__(self, split='train'):\n","        super().__init__()\n","        self.examples = []\n","\n","        df = pd.read_csv(f'/kaggle/input/neurips-2023-machine-unlearning/{split}.csv')\n","        df['image_path'] = df['image_id'].apply(\n","            lambda x: os.path.join('/kaggle/input/neurips-2023-machine-unlearning/', 'images', x.split('-')[0], x.split('-')[1] + '.png'))\n","        df = df.sort_values(by='image_path')\n","        df.apply(lambda row: self.examples.append(load_example(row)), axis=1)\n","        if len(self.examples) == 0:\n","            raise ValueError('No examples.')\n","\n","    def __len__(self):\n","        return len(self.examples)\n","\n","    def __getitem__(self, idx):\n","        example = self.examples[idx]\n","        image = example['image']\n","        image = image.to(torch.float32)\n","        example['image'] = image\n","        return example\n","\n","\n","def get_dataset(batch_size):\n","    '''Get the dataset.'''\n","    retain_ds = HiddenDataset(split='retain')\n","    forget_ds = HiddenDataset(split='forget')\n","    val_ds = HiddenDataset(split='validation')\n","\n","    retain_loader = DataLoader(retain_ds, batch_size=batch_size, shuffle=True)\n","    forget_loader = DataLoader(forget_ds, batch_size=batch_size, shuffle=True)\n","    validation_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n","\n","    return retain_loader, forget_loader, validation_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T21:43:34.187458Z","iopub.status.busy":"2023-09-25T21:43:34.187099Z","iopub.status.idle":"2023-09-25T21:43:34.215127Z","shell.execute_reply":"2023-09-25T21:43:34.214138Z","shell.execute_reply.started":"2023-09-25T21:43:34.187418Z"},"trusted":true},"outputs":[],"source":["class ParameterPerturber:\n","    def __init__(\n","        self,\n","        model,\n","        opt,\n","        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n","        parameters=None,\n","    ):\n","        self.model = model\n","        self.opt = opt\n","        self.device = device\n","        self.alpha = None\n","        self.xmin = None\n","\n","        # print(parameters)\n","        self.lower_bound = parameters[\"lower_bound\"]\n","        self.exponent = parameters[\"exponent\"]\n","        self.magnitude_diff = parameters[\"magnitude_diff\"]  # unused\n","        self.min_layer = parameters[\"min_layer\"]\n","        self.max_layer = parameters[\"max_layer\"]\n","        self.forget_threshold = parameters[\"forget_threshold\"]\n","        self.dampening_constant = parameters[\"dampening_constant\"]\n","        self.selection_weighting = parameters[\"selection_weighting\"]\n","\n","    def get_layer_num(self, layer_name: str) -> int:\n","        layer_id = layer_name.split(\".\")[1]\n","        if layer_id.isnumeric():\n","            return int(layer_id)\n","        else:\n","            return -1\n","\n","    def zerolike_params_dict(self, model: torch.nn) -> Dict[str, torch.Tensor]:\n","        \"\"\"\n","        Taken from: Avalanche: an End-to-End Library for Continual Learning - https://github.com/ContinualAI/avalanche\n","        Returns a dict like named_parameters(), with zeroed-out parameter valuse\n","        Parameters:\n","        model (torch.nn): model to get param dict from\n","        Returns:\n","        dict(str,torch.Tensor): dict of zero-like params\n","        \"\"\"\n","        return dict(\n","            [\n","                (k, torch.zeros_like(p, device=p.device))\n","                for k, p in model.named_parameters()\n","            ]\n","        )\n","\n","    def fulllike_params_dict(\n","        self, model: torch.nn, fill_value, as_tensor: bool = False\n","    ) -> Dict[str, torch.Tensor]:\n","        \"\"\"\n","        Returns a dict like named_parameters(), with parameter values replaced with fill_value\n","\n","        Parameters:\n","        model (torch.nn): model to get param dict from\n","        fill_value: value to fill dict with\n","        Returns:\n","        dict(str,torch.Tensor): dict of named_parameters() with filled in values\n","        \"\"\"\n","\n","        def full_like_tensor(fillval, shape: list) -> list:\n","            \"\"\"\n","            recursively builds nd list of shape shape, filled with fillval\n","            Parameters:\n","            fillval: value to fill matrix with\n","            shape: shape of target tensor\n","            Returns:\n","            list of shape shape, filled with fillval at each index\n","            \"\"\"\n","            if len(shape) > 1:\n","                fillval = full_like_tensor(fillval, shape[1:])\n","            tmp = [fillval for _ in range(shape[0])]\n","            return tmp\n","\n","        dictionary = {}\n","\n","        for n, p in model.named_parameters():\n","            _p = (\n","                torch.tensor(full_like_tensor(fill_value, p.shape), device=self.device)\n","                if as_tensor\n","                else full_like_tensor(fill_value, p.shape)\n","            )\n","            dictionary[n] = _p\n","        return dictionary\n","\n","    def subsample_dataset(self, dataset: dataset, sample_perc: float) -> Subset:\n","        \"\"\"\n","        Take a subset of the dataset\n","\n","        Parameters:\n","        dataset (dataset): dataset to be subsampled\n","        sample_perc (float): percentage of dataset to sample. range(0,1)\n","        Returns:\n","        Subset (float): requested subset of the dataset\n","        \"\"\"\n","        sample_idxs = np.arange(0, len(dataset), step=int((1 / sample_perc)))\n","        return Subset(dataset, sample_idxs)\n","\n","    def split_dataset_by_class(self, dataset: dataset) -> List[Subset]:\n","        \"\"\"\n","        Split dataset into list of subsets\n","            each idx corresponds to samples from that class\n","\n","        Parameters:\n","        dataset (dataset): dataset to be split\n","        Returns:\n","        subsets (List[Subset]): list of subsets of the dataset,\n","            each containing only the samples belonging to that class\n","        \"\"\"\n","        n_classes = len(set([target for _, target in dataset]))\n","        subset_idxs = [[] for _ in range(n_classes)]\n","        for idx, (x, y) in enumerate(dataset):\n","            subset_idxs[y].append(idx)\n","\n","        return [Subset(dataset, subset_idxs[idx]) for idx in range(n_classes)]\n","\n","    def calc_importance(self, dataloader: DataLoader) -> Dict[str, torch.Tensor]:\n","        \"\"\"\n","        Adapated from: Avalanche: an End-to-End Library for Continual Learning - https://github.com/ContinualAI/avalanche\n","        Calculate per-parameter, importance\n","            returns a dictionary [param_name: list(importance per parameter)]\n","        Parameters:\n","        DataLoader (DataLoader): DataLoader to be iterated over\n","        Returns:\n","        importances (dict(str, torch.Tensor([]))): named_parameters-like dictionary containing list of importances for each parameter\n","        \"\"\"\n","        criterion = nn.CrossEntropyLoss()\n","        importances = self.zerolike_params_dict(self.model)\n","        for sample in dataloader:\n","            x = sample[\"image\"]\n","            y = sample[\"age_group\"]\n","            x, y = x.to(self.device), y.to(self.device)\n","            self.opt.zero_grad()\n","            out = self.model(x)\n","            loss = criterion(out, y)\n","            loss.backward()\n","\n","            for (k1, p), (k2, imp) in zip(\n","                self.model.named_parameters(), importances.items()\n","            ):\n","                if p.grad is not None:\n","                    imp.data += p.grad.data.clone().pow(2)\n","\n","        # average over mini batch length\n","        for _, imp in importances.items():\n","            imp.data /= float(len(dataloader))\n","        return importances\n","\n","    def modify_weight(\n","        self,\n","        original_importance: List[Dict[str, torch.Tensor]],\n","        forget_importance: List[Dict[str, torch.Tensor]],\n","    ) -> None:\n","        \"\"\"\n","        Perturb weights based on the SSD equations given in the paper\n","        Parameters:\n","        original_importance (List[Dict[str, torch.Tensor]]): list of importances for original dataset\n","        forget_importance (List[Dict[str, torch.Tensor]]): list of importances for forget sample\n","        threshold (float): value to multiply original imp by to determine memorization.\n","\n","        Returns:\n","        None\n","\n","        \"\"\"\n","\n","        with torch.no_grad():\n","            for (n, p), (oimp_n, oimp), (fimp_n, fimp) in zip(\n","                self.model.named_parameters(),\n","                original_importance.items(),\n","                forget_importance.items(),\n","            ):\n","                # Synapse Selection with parameter alpha\n","                oimp_norm = oimp.mul(self.selection_weighting)\n","                locations = torch.where(fimp > oimp_norm)\n","\n","                # Synapse Dampening with parameter lambda\n","                weight = ((oimp.mul(self.dampening_constant)).div(fimp)).pow(\n","                    self.exponent\n","                )\n","                update = weight[locations]\n","                # Bound by 1 to prevent parameter values to increase.\n","                min_locs = torch.where(update > self.lower_bound)\n","                update[min_locs] = self.lower_bound\n","                p[locations] = p[locations].mul(update)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T21:43:34.217578Z","iopub.status.busy":"2023-09-25T21:43:34.216371Z","iopub.status.idle":"2023-09-25T21:43:34.228979Z","shell.execute_reply":"2023-09-25T21:43:34.227965Z","shell.execute_reply.started":"2023-09-25T21:43:34.217544Z"},"trusted":true},"outputs":[],"source":["def unlearning(\n","    net, \n","    retain_loader, \n","    forget_loader, \n","    val_loader):\n","    \n","    epochs = 1\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.SGD(net.parameters(), lr=0.001,\n","                      momentum=0.9, weight_decay=0)\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n","        optimizer, T_max=epochs)\n","\n","    alpha = 1 # alpha in the paper\n","    lambda_ = 10 # lambda in the paper\n","    selection_weighting = 10 * alpha\n","\n","    parameters = {\n","        \"lower_bound\": 1,\n","        \"exponent\": 1,\n","        \"magnitude_diff\": None,\n","        \"min_layer\": -1,\n","        \"max_layer\": -1,\n","        \"forget_threshold\": 1,\n","        \"dampening_constant\": lambda_,\n","        \"selection_weighting\": selection_weighting,\n","    }\n","    \n","    full_train_dl = DataLoader(\n","    ConcatDataset((retain_loader.dataset, forget_loader.dataset)),\n","    batch_size=64,\n","    )\n","\n","    pdr = ParameterPerturber(net, optimizer, DEVICE, parameters)\n","\n","    net = net.eval()\n","\n","    sample_importances = pdr.calc_importance(forget_loader)\n","    original_importances = pdr.calc_importance(full_train_dl)\n","    pdr.modify_weight(original_importances, sample_importances)\n","\n","    net.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T21:43:34.232382Z","iopub.status.busy":"2023-09-25T21:43:34.231320Z","iopub.status.idle":"2023-09-25T21:43:34.252717Z","shell.execute_reply":"2023-09-25T21:43:34.251639Z","shell.execute_reply.started":"2023-09-25T21:43:34.232348Z"},"trusted":true},"outputs":[],"source":["if os.path.exists('/kaggle/input/neurips-2023-machine-unlearning/empty.txt'):\n","    # mock submission\n","    subprocess.run('touch submission.zip', shell=True)\n","else:\n","    \n","    # Note: it's really important to create the unlearned checkpoints outside of the working directory \n","    # as otherwise this notebook may fail due to running out of disk space.\n","    # The below code saves them in /kaggle/tmp to avoid that issue.\n","    \n","    os.makedirs('/kaggle/tmp', exist_ok=True)\n","    retain_loader, forget_loader, validation_loader = get_dataset(64)\n","    net = resnet18(weights=None, num_classes=10)\n","    net.to(DEVICE)\n","    for i in range(512):\n","        net.load_state_dict(torch.load('/kaggle/input/neurips-2023-machine-unlearning/original_model.pth'))\n","        unlearning(net, retain_loader, forget_loader, validation_loader)\n","        state = net.state_dict()\n","        torch.save(state, f'/kaggle/tmp/unlearned_checkpoint_{i}.pth')\n","        \n","    # Ensure that submission.zip will contain exactly 512 checkpoints \n","    # (if this is not the case, an exception will be thrown).\n","    unlearned_ckpts = os.listdir('/kaggle/tmp')\n","    if len(unlearned_ckpts) != 512:\n","        raise RuntimeError('Expected exactly 512 checkpoints. The submission will throw an exception otherwise.')\n","        \n","    subprocess.run('zip submission.zip /kaggle/tmp/*.pth', shell=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
