{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-10-29T18:13:48.694433Z","iopub.status.busy":"2023-10-29T18:13:48.693608Z","iopub.status.idle":"2023-10-29T18:13:52.948297Z","shell.execute_reply":"2023-10-29T18:13:52.947411Z","shell.execute_reply.started":"2023-10-29T18:13:48.694401Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import os\n","import gc\n","import math\n","import random\n","import numpy as np\n","import copy\n","import subprocess\n","from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n","from sklearn import linear_model, model_selection\n","\n","import pandas as pd\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils import prune\n","import torch.optim as optim\n","from torchvision.models import resnet18\n","from torch.utils.data import DataLoader, Dataset, TensorDataset, Subset\n","from torchvision.models.feature_extraction import create_feature_extractor\n","\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' "]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-10-29T18:13:52.950386Z","iopub.status.busy":"2023-10-29T18:13:52.950007Z","iopub.status.idle":"2023-10-29T18:13:52.954502Z","shell.execute_reply":"2023-10-29T18:13:52.953674Z","shell.execute_reply.started":"2023-10-29T18:13:52.950362Z"},"trusted":true},"outputs":[],"source":["# It's really important to add an accelerator to your notebook, as otherwise the submission will fail.\n","# We recomment using the P100 GPU rather than T4 as it's faster and will increase the chances of passing the time cut-off threshold.\n","\n","if DEVICE != 'cuda':\n","    raise RuntimeError('Make sure you have added an accelerator to your notebook; the submission will fail otherwise!')"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-10-29T18:13:52.956332Z","iopub.status.busy":"2023-10-29T18:13:52.956079Z","iopub.status.idle":"2023-10-29T18:13:52.978873Z","shell.execute_reply":"2023-10-29T18:13:52.978023Z","shell.execute_reply.started":"2023-10-29T18:13:52.956309Z"},"trusted":true},"outputs":[],"source":["# Helper functions for loading the hidden dataset.\n","\n","def load_example(df_row):\n","    image = torchvision.io.read_image(df_row['image_path'])\n","    result = {\n","        'image': image,\n","        'image_id': df_row['image_id'],\n","        'age_group': df_row['age_group'],\n","        'age': df_row['age'],\n","        'person_id': df_row['person_id']\n","    }\n","    return result\n","\n","\n","class HiddenDataset(Dataset):\n","    '''The hidden dataset.'''\n","    def __init__(self, split='train'):\n","        super().__init__()\n","        self.examples = []\n","\n","        df = pd.read_csv(f'/kaggle/input/neurips-2023-machine-unlearning/{split}.csv')\n","        df['image_path'] = df['image_id'].apply(\n","            lambda x: os.path.join('/kaggle/input/neurips-2023-machine-unlearning/', 'images', x.split('-')[0], x.split('-')[1] + '.png'))\n","        df = df.sample(frac=1).reset_index(drop=True)\n","        df.apply(lambda row: self.examples.append(load_example(row)), axis=1)\n","        if len(self.examples) == 0:\n","            raise ValueError('No examples.')\n","\n","    def __len__(self):\n","        return len(self.examples)\n","\n","    def __getitem__(self, idx):\n","        example = self.examples[idx]\n","        image = example['image']\n","        image = image.to(torch.float32)\n","        example['image'] = image\n","        return example\n","\n","\n","def get_dataset(batch_size):\n","    '''Get the dataset.'''\n","    retain_ds = HiddenDataset(split='retain')\n","    forget_ds = HiddenDataset(split='forget')\n","    val_ds = HiddenDataset(split='validation')\n","\n","    retain_loader = DataLoader(retain_ds, batch_size=batch_size, shuffle=True)\n","    forget_loader = DataLoader(forget_ds, batch_size=batch_size, shuffle=True)\n","    validation_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n","\n","    return retain_loader, forget_loader, validation_loader"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-10-29T18:13:52.980283Z","iopub.status.busy":"2023-10-29T18:13:52.979989Z","iopub.status.idle":"2023-10-29T18:13:52.991796Z","shell.execute_reply":"2023-10-29T18:13:52.990987Z","shell.execute_reply.started":"2023-10-29T18:13:52.980226Z"},"trusted":true},"outputs":[],"source":["def accuracy(net, loader):\n","    \"\"\"Return accuracy on a dataset given by the data loader.\"\"\"\n","    correct = 0\n","    total = 0\n","    for sample in loader:\n","        inputs = sample[\"image\"]\n","        targets = sample[\"age_group\"]\n","        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n","        outputs = net(inputs)\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","    return correct / total"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-10-29T18:13:52.994545Z","iopub.status.busy":"2023-10-29T18:13:52.993956Z","iopub.status.idle":"2023-10-29T18:13:53.006342Z","shell.execute_reply":"2023-10-29T18:13:53.005462Z","shell.execute_reply.started":"2023-10-29T18:13:52.994499Z"},"trusted":true},"outputs":[],"source":["# Function to update learning rate\n","def adjust_learning_rate(optimizer, current_batch, total_batches, initial_lr):\n","    \"\"\"Sets the learning rate for warmup over total_batches\"\"\"\n","    lr = initial_lr * (current_batch / total_batches)\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-10-29T18:13:53.020478Z","iopub.status.busy":"2023-10-29T18:13:53.020164Z","iopub.status.idle":"2023-10-29T18:13:53.036636Z","shell.execute_reply":"2023-10-29T18:13:53.035789Z","shell.execute_reply.started":"2023-10-29T18:13:53.020446Z"},"trusted":true},"outputs":[],"source":["def get_embeddings(\n","    net, \n","    retain_loader,\n","    val_loader\n","):\n","    \n","    '''\n","    Feature extraction\n","    '''\n","    \n","    feat_extractor = create_feature_extractor(net, {'avgpool': 'feat1'})\n","    \n","    '''\n","    Get class weights\n","    '''\n","    \n","    # Retain logits\n","    data = np.empty((len(retain_loader.dataset), 513), dtype=object)\n","    idx = 0\n","    \n","    with torch.no_grad():\n","        for sample in retain_loader:\n","            # Get logits\n","            targets = sample[\"age_group\"]\n","            \n","            # Feature extraction\n","            inputs = sample[\"image\"]\n","            person_id = sample[\"person_id\"]\n","            outputs = feat_extractor(inputs.to(DEVICE))['feat1']\n","            feats = torch.flatten(outputs, start_dim=1)\n","        \n","            for i in range(len(targets)):\n","                data[idx] = [targets[i]] + feats[i].cpu().numpy().tolist()\n","                idx +=1\n","       \n","    columns = ['unique_id'] + [f'feat_{i}' for i in range(512)]\n","    embeddings_retain_df = pd.DataFrame(data, columns=columns)\n","    \n","\n","    # Val logits\n","    data = np.empty((len(val_loader.dataset), 513), dtype=object)\n","    idx = 0\n","    \n","    with torch.no_grad():\n","        for sample in val_loader:\n","            # Get logits\n","            targets = sample[\"age_group\"]\n","            \n","            # Feature extraction\n","            inputs = sample[\"image\"]\n","            person_id = sample[\"person_id\"]\n","            outputs = feat_extractor(inputs.to(DEVICE))['feat1']\n","            feats = torch.flatten(outputs, start_dim=1)\n","        \n","            for i in range(len(targets)):\n","                data[idx] = [str(person_id[i])] + feats[i].cpu().numpy().tolist()\n","                idx +=1\n","\n","    columns = ['unique_id'] + [f'feat_{i}' for i in range(512)]\n","    embeddings_val_df = pd.DataFrame(data, columns=columns)\n","    \n","\n","    return embeddings_retain_df, embeddings_val_df"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-10-29T18:13:53.113436Z","iopub.status.busy":"2023-10-29T18:13:53.112418Z","iopub.status.idle":"2023-10-29T18:13:53.126367Z","shell.execute_reply":"2023-10-29T18:13:53.125549Z","shell.execute_reply.started":"2023-10-29T18:13:53.113410Z"},"trusted":true},"outputs":[],"source":["# Contrastive Loss\n","class ContrastiveLoss(nn.Module):\n","    def __init__(self, margin=1.0):\n","        super(ContrastiveLoss, self).__init__()\n","        self.margin = margin\n","    \n","    def forward(self, output1, output2, label):\n","        euclidean_distance = nn.functional.pairwise_distance(output1, output2)\n","        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) + (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n","        return loss_contrastive"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-10-29T18:13:53.129830Z","iopub.status.busy":"2023-10-29T18:13:53.129527Z","iopub.status.idle":"2023-10-29T18:13:53.137867Z","shell.execute_reply":"2023-10-29T18:13:53.136959Z","shell.execute_reply.started":"2023-10-29T18:13:53.129801Z"},"trusted":true},"outputs":[],"source":["# Extract feature and pooling layers to create a Custom Model\n","class CustomResNet18(nn.Module):\n","    def __init__(self, original_model):\n","        super(CustomResNet18, self).__init__()\n","\n","        # Extract features and pooling layers\n","        self.features = nn.Sequential(*list(original_model.children())[:-2])\n","        self.pooling = list(original_model.children())[-2]\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.pooling(x)\n","        x = torch.squeeze(x)\n","        return x"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-10-29T18:13:53.139240Z","iopub.status.busy":"2023-10-29T18:13:53.138984Z","iopub.status.idle":"2023-10-29T18:13:53.152991Z","shell.execute_reply":"2023-10-29T18:13:53.152173Z","shell.execute_reply.started":"2023-10-29T18:13:53.139217Z"},"trusted":true},"outputs":[],"source":["def contrastive_unlearning(net, forget_loader, embeddings_retain_df, embeddings_val_df, LR=1e-3, max_num_steps=3):\n","    \n","    custom_model = CustomResNet18(net).to(DEVICE)\n","    criterion = ContrastiveLoss()\n","    optimizer = optim.AdamW(custom_model.parameters(), lr=LR)\n","    \n","    for i, batch in enumerate(forget_loader):\n","        custom_model.train()\n","        optimizer.zero_grad()\n","        inputs = batch['image'].to(DEVICE)\n","        targets = batch['age_group']\n","        person_ids = batch['person_id']\n","\n","        # Forward pass to get embeddings for the forget_batch\n","        forget_embeddings = custom_model(inputs)\n","\n","        positive_pairs = []\n","        negative_pairs = []\n","\n","        with torch.no_grad():  # Disable gradient computation to save memory\n","            \n","            # Fetch Positive Pairs\n","            for pid in person_ids:\n","                ''' TODO '''\n","\n","            # Convert to tensors for ease of computation\n","            positive_pairs = torch.stack(positive_pairs).to(DEVICE)\n","\n","            # Fetch Negative Pairs\n","            for tgt in targets:\n","                ''' TODO '''\n","\n","            # Convert to tensors for ease of computation\n","            negative_pairs = torch.stack(negative_pairs).to(DEVICE)\n","\n","        # Compute Contrastive Loss\n","        positive_loss = criterion(forget_embeddings, positive_pairs, torch.zeros(positive_pairs.shape[0]).to(DEVICE))\n","        negative_loss = criterion(forget_embeddings, negative_pairs, torch.ones(negative_pairs.shape[0]).to(DEVICE))\n","\n","        # Total loss\n","        loss = positive_loss + negative_loss\n","\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if i==max_num_steps:\n","            break"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-10-29T18:13:53.154661Z","iopub.status.busy":"2023-10-29T18:13:53.154367Z","iopub.status.idle":"2023-10-29T18:13:53.175835Z","shell.execute_reply":"2023-10-29T18:13:53.175059Z","shell.execute_reply.started":"2023-10-29T18:13:53.154638Z"},"trusted":true},"outputs":[],"source":["if os.path.exists('/kaggle/input/neurips-2023-machine-unlearning/empty.txt'):\n","    # mock submission\n","    subprocess.run('touch submission.zip', shell=True)\n","else:\n","    os.makedirs('/kaggle/tmp', exist_ok=True)\n","     \n","    \n","    '''\n","    Get data loaders\n","    '''\n","    batch_size = 32\n","    retain_loader, forget_loader, validation_loader = get_dataset(batch_size)\n","    \n","    \n","    \n","    '''\n","    Contrastive\n","    '''\n","    \n","    net = resnet18(weights=None, num_classes=10)\n","    net.to(DEVICE)\n","    net.load_state_dict(torch.load('/kaggle/input/neurips-2023-machine-unlearning/original_model.pth'))\n","    \n","    embeddings_retain_df, embeddings_val_df = get_embeddings(net, retain_loader, val_loader)\n","\n","\n","    '''\n","    Loop\n","    '''\n","    \n","    for i in range(512):\n","        net = resnet18(weights=None, num_classes=10)\n","        net.to(DEVICE)\n","        net.load_state_dict(torch.load('/kaggle/input/neurips-2023-machine-unlearning/original_model.pth'))\n","        contrastive_unlearning(net, forget_loader, embeddings_retain_df, embeddings_val_df, LR=1e-3, max_num_steps=3)\n","        state = net.state_dict()\n","        torch.save(state, f'/kaggle/tmp/unlearned_checkpoint_{i}.pth')\n","        gc.collect()\n","    \n","    # In the tmp/ folder, there will be 512 checkpoints to submit + 1 for validation early stop that doesn't get zipped\n","    subprocess.run('zip submission.zip /kaggle/tmp/unlearned_*.pth', shell=True)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
