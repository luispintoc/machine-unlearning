{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc71ef1a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-30T01:12:25.241539Z",
     "iopub.status.busy": "2023-10-30T01:12:25.241113Z",
     "iopub.status.idle": "2023-10-30T01:12:30.288332Z",
     "shell.execute_reply": "2023-10-30T01:12:30.287333Z"
    },
    "papermill": {
     "duration": 5.055036,
     "end_time": "2023-10-30T01:12:30.290630",
     "exception": false,
     "start_time": "2023-10-30T01:12:25.235594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import subprocess\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn import linear_model, model_selection\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import prune\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, Subset\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4926dccd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T01:12:30.304214Z",
     "iopub.status.busy": "2023-10-30T01:12:30.303101Z",
     "iopub.status.idle": "2023-10-30T01:12:30.308601Z",
     "shell.execute_reply": "2023-10-30T01:12:30.307762Z"
    },
    "papermill": {
     "duration": 0.014005,
     "end_time": "2023-10-30T01:12:30.310695",
     "exception": false,
     "start_time": "2023-10-30T01:12:30.296690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# It's really important to add an accelerator to your notebook, as otherwise the submission will fail.\n",
    "# We recomment using the P100 GPU rather than T4 as it's faster and will increase the chances of passing the time cut-off threshold.\n",
    "\n",
    "if DEVICE != 'cuda':\n",
    "    raise RuntimeError('Make sure you have added an accelerator to your notebook; the submission will fail otherwise!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d7a97c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T01:12:30.322648Z",
     "iopub.status.busy": "2023-10-30T01:12:30.322160Z",
     "iopub.status.idle": "2023-10-30T01:12:30.339373Z",
     "shell.execute_reply": "2023-10-30T01:12:30.338491Z"
    },
    "papermill": {
     "duration": 0.026434,
     "end_time": "2023-10-30T01:12:30.341700",
     "exception": false,
     "start_time": "2023-10-30T01:12:30.315266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions for loading the hidden dataset.\n",
    "\n",
    "def load_example(df_row):\n",
    "    image = torchvision.io.read_image(df_row['image_path'])\n",
    "    result = {\n",
    "        'image': image,\n",
    "        'image_id': df_row['image_id'],\n",
    "        'age_group': df_row['age_group'],\n",
    "        'age': df_row['age'],\n",
    "        'person_id': df_row['person_id']\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "class HiddenDataset(Dataset):\n",
    "    '''The hidden dataset.'''\n",
    "    def __init__(self, split='train'):\n",
    "        super().__init__()\n",
    "        self.examples = []\n",
    "\n",
    "        df = pd.read_csv(f'/kaggle/input/neurips-2023-machine-unlearning/{split}.csv')\n",
    "        df['image_path'] = df['image_id'].apply(\n",
    "            lambda x: os.path.join('/kaggle/input/neurips-2023-machine-unlearning/', 'images', x.split('-')[0], x.split('-')[1] + '.png'))\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "        df.apply(lambda row: self.examples.append(load_example(row)), axis=1)\n",
    "        if len(self.examples) == 0:\n",
    "            raise ValueError('No examples.')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        image = example['image']\n",
    "        image = image.to(torch.float32)\n",
    "        example['image'] = image\n",
    "        return example\n",
    "\n",
    "\n",
    "def get_dataset(batch_size):\n",
    "    '''Get the dataset.'''\n",
    "    retain_ds = HiddenDataset(split='retain')\n",
    "    forget_ds = HiddenDataset(split='forget')\n",
    "    val_ds = HiddenDataset(split='validation')\n",
    "\n",
    "    retain_loader = DataLoader(retain_ds, batch_size=batch_size, shuffle=True)\n",
    "    forget_loader = DataLoader(forget_ds, batch_size=batch_size, shuffle=True)\n",
    "    validation_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return retain_loader, forget_loader, validation_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51829943",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T01:12:30.355356Z",
     "iopub.status.busy": "2023-10-30T01:12:30.354964Z",
     "iopub.status.idle": "2023-10-30T01:12:30.362189Z",
     "shell.execute_reply": "2023-10-30T01:12:30.361385Z"
    },
    "papermill": {
     "duration": 0.017082,
     "end_time": "2023-10-30T01:12:30.364080",
     "exception": false,
     "start_time": "2023-10-30T01:12:30.346998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy(net, loader):\n",
    "    \"\"\"Return accuracy on a dataset given by the data loader.\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for sample in loader:\n",
    "        inputs = sample[\"image\"]\n",
    "        targets = sample[\"age_group\"]\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "966347c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T01:12:30.372713Z",
     "iopub.status.busy": "2023-10-30T01:12:30.372469Z",
     "iopub.status.idle": "2023-10-30T01:12:30.377036Z",
     "shell.execute_reply": "2023-10-30T01:12:30.376184Z"
    },
    "papermill": {
     "duration": 0.010991,
     "end_time": "2023-10-30T01:12:30.378905",
     "exception": false,
     "start_time": "2023-10-30T01:12:30.367914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to update learning rate\n",
    "def adjust_learning_rate(optimizer, current_batch, total_batches, initial_lr):\n",
    "    \"\"\"Sets the learning rate for warmup over total_batches\"\"\"\n",
    "    lr = initial_lr * (current_batch / total_batches)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fbbf511",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T01:12:30.387707Z",
     "iopub.status.busy": "2023-10-30T01:12:30.387451Z",
     "iopub.status.idle": "2023-10-30T01:12:30.398205Z",
     "shell.execute_reply": "2023-10-30T01:12:30.397566Z"
    },
    "papermill": {
     "duration": 0.017409,
     "end_time": "2023-10-30T01:12:30.400162",
     "exception": false,
     "start_time": "2023-10-30T01:12:30.382753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embeddings(\n",
    "    net, \n",
    "    retain_loader,\n",
    "    val_loader\n",
    "):\n",
    "    \n",
    "    '''\n",
    "    Feature extraction\n",
    "    '''\n",
    "    \n",
    "    feat_extractor = create_feature_extractor(net, {'avgpool': 'feat1'})\n",
    "    \n",
    "    '''\n",
    "    Get class weights\n",
    "    '''\n",
    "    \n",
    "    # Retain logits\n",
    "    data = np.empty((len(retain_loader.dataset), 513), dtype=object)\n",
    "    idx = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sample in retain_loader:\n",
    "            # Get logits\n",
    "            targets = sample[\"age_group\"]\n",
    "            \n",
    "            # Feature extraction\n",
    "            inputs = sample[\"image\"]\n",
    "            person_id = sample[\"person_id\"]\n",
    "            outputs = feat_extractor(inputs.to(DEVICE))['feat1']\n",
    "            feats = torch.flatten(outputs, start_dim=1)\n",
    "        \n",
    "            for i in range(len(targets)):\n",
    "                data[idx] = [targets[i].item()] + feats[i].cpu().numpy().tolist()\n",
    "                idx +=1\n",
    "       \n",
    "    columns = ['unique_id'] + [f'feat_{i}' for i in range(512)]\n",
    "    embeddings_retain_df = pd.DataFrame(data, columns=columns)\n",
    "    \n",
    "\n",
    "    # Val logits\n",
    "    data = np.empty((len(val_loader.dataset), 513), dtype=object)\n",
    "    idx = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sample in val_loader:\n",
    "            # Get logits\n",
    "            targets = sample[\"age_group\"]\n",
    "            \n",
    "            # Feature extraction\n",
    "            inputs = sample[\"image\"]\n",
    "            person_id = sample[\"person_id\"]\n",
    "            outputs = feat_extractor(inputs.to(DEVICE))['feat1']\n",
    "            feats = torch.flatten(outputs, start_dim=1)\n",
    "        \n",
    "            for i in range(len(targets)):\n",
    "                data[idx] = [str(person_id[i])] + feats[i].cpu().numpy().tolist()\n",
    "                idx +=1\n",
    "\n",
    "    columns = ['unique_id'] + [f'feat_{i}' for i in range(512)]\n",
    "    embeddings_val_df = pd.DataFrame(data, columns=columns)\n",
    "    \n",
    "\n",
    "    return embeddings_retain_df, embeddings_val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff279fe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T01:12:30.408759Z",
     "iopub.status.busy": "2023-10-30T01:12:30.408508Z",
     "iopub.status.idle": "2023-10-30T01:12:30.414282Z",
     "shell.execute_reply": "2023-10-30T01:12:30.413428Z"
    },
    "papermill": {
     "duration": 0.012299,
     "end_time": "2023-10-30T01:12:30.416250",
     "exception": false,
     "start_time": "2023-10-30T01:12:30.403951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Contrastive Loss\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = nn.functional.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) + (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25c32504",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T01:12:30.425100Z",
     "iopub.status.busy": "2023-10-30T01:12:30.424862Z",
     "iopub.status.idle": "2023-10-30T01:12:30.430466Z",
     "shell.execute_reply": "2023-10-30T01:12:30.429674Z"
    },
    "papermill": {
     "duration": 0.011904,
     "end_time": "2023-10-30T01:12:30.432254",
     "exception": false,
     "start_time": "2023-10-30T01:12:30.420350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract feature and pooling layers to create a Custom Model\n",
    "class CustomResNet18(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(CustomResNet18, self).__init__()\n",
    "\n",
    "        # Extract features and pooling layers\n",
    "        self.features = nn.Sequential(*list(original_model.children())[:-2])\n",
    "        self.pooling = list(original_model.children())[-2]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pooling(x)\n",
    "        x = torch.squeeze(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c213116a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T01:12:30.440978Z",
     "iopub.status.busy": "2023-10-30T01:12:30.440748Z",
     "iopub.status.idle": "2023-10-30T01:12:30.451982Z",
     "shell.execute_reply": "2023-10-30T01:12:30.451153Z"
    },
    "papermill": {
     "duration": 0.017807,
     "end_time": "2023-10-30T01:12:30.453818",
     "exception": false,
     "start_time": "2023-10-30T01:12:30.436011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def contrastive_unlearning(net, forget_loader, grouped_retain_df, grouped_val_df, LR=1e-3, max_num_steps=3):\n",
    "    \n",
    "    custom_model = CustomResNet18(net).to(DEVICE)\n",
    "    criterion = ContrastiveLoss()\n",
    "    optimizer = optim.AdamW(custom_model.parameters(), lr=LR)\n",
    "    \n",
    "    for i, batch in enumerate(forget_loader):\n",
    "        custom_model.train()\n",
    "        optimizer.zero_grad()\n",
    "        inputs = batch['image'].to(DEVICE)\n",
    "        targets = batch['age_group']\n",
    "        person_ids = batch['person_id']\n",
    "\n",
    "        # Forward pass to get embeddings for the forget_batch\n",
    "        forget_embeddings = custom_model(inputs)\n",
    "\n",
    "        positive_pairs = []\n",
    "        negative_pairs = []\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation to save memory\n",
    "            \n",
    "            # Fetch Positive Pairs\n",
    "            for index, pid in enumerate(person_ids):\n",
    "                candidate_embeddings = grouped_val_df.get(str(pid), None)\n",
    "                if candidate_embeddings is not None:  # If a positive pair exists\n",
    "                    selected_embedding = shuffle(candidate_embeddings, n_samples=1)[0]  # Randomly select one\n",
    "                else:  # Fallback to using the instance's own embedding\n",
    "                    selected_embedding = forget_embeddings[index].cpu().detach().numpy()\n",
    "\n",
    "                positive_pairs.append(torch.tensor(selected_embedding.astype(float)).float())\n",
    "\n",
    "\n",
    "            # Convert to tensors for ease of computation\n",
    "            positive_pairs = torch.stack(positive_pairs).to(DEVICE)\n",
    "\n",
    "            # Fetch Negative Pairs\n",
    "            for tgt in targets.cpu().numpy():\n",
    "                candidate_embeddings = grouped_retain_df.get(tgt, None)\n",
    "                if candidate_embeddings is not None:\n",
    "                    selected_embedding = shuffle(candidate_embeddings, n_samples=1)[0]  # Randomly select one\n",
    "                    negative_pairs.append(torch.tensor(selected_embedding.astype(float)).float())\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "\n",
    "            # Convert to tensors for ease of computation\n",
    "            negative_pairs = torch.stack(negative_pairs).to(DEVICE)\n",
    "\n",
    "        # Compute Contrastive Loss\n",
    "        positive_loss = criterion(forget_embeddings, positive_pairs, torch.zeros(positive_pairs.shape[0]).to(DEVICE))\n",
    "        negative_loss = criterion(forget_embeddings, negative_pairs, torch.ones(negative_pairs.shape[0]).to(DEVICE))\n",
    "\n",
    "        # Total loss\n",
    "        loss = positive_loss # + negative_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i==max_num_steps:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76abec1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T01:12:30.462682Z",
     "iopub.status.busy": "2023-10-30T01:12:30.461990Z",
     "iopub.status.idle": "2023-10-30T01:12:30.470031Z",
     "shell.execute_reply": "2023-10-30T01:12:30.469055Z"
    },
    "papermill": {
     "duration": 0.014276,
     "end_time": "2023-10-30T01:12:30.471793",
     "exception": false,
     "start_time": "2023-10-30T01:12:30.457517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrain_step(net, retain_loader, retain_class_weights=None, LR=5e-5, max_num_steps=3):\n",
    "\n",
    "    initial_retain_lr = LR\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_retain = optim.SGD(net.parameters(), lr=initial_retain_lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    warmup_current_batch = 0\n",
    "    warmup_batches = math.ceil(0.4*len(retain_loader.dataset))\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    for counter, sample in enumerate(retain_loader):\n",
    "\n",
    "        inputs = sample[\"image\"]\n",
    "        targets = sample[\"age_group\"]\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        warmup_current_batch += 1\n",
    "\n",
    "        # Warm-up for the first 'warmup_batches' batches\n",
    "#         if warmup_current_batch <= warmup_batches:\n",
    "#             adjust_learning_rate(optimizer_retain, warmup_current_batch, warmup_batches, initial_retain_lr)\n",
    "\n",
    "        optimizer_retain.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = net(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        criterion = nn.CrossEntropyLoss(weight=retain_class_weights, label_smoothing=0.0)\n",
    "        classification_loss = criterion(logits, targets)\n",
    "        loss = classification_loss\n",
    "        loss.backward()\n",
    "        optimizer_retain.step()\n",
    "\n",
    "        if counter==max_num_steps:\n",
    "            break\n",
    "        \n",
    "#     torch.save({\n",
    "#         'net': net.state_dict(),\n",
    "#     }, f'/kaggle/tmp/temp_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98d40edc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T01:12:30.480254Z",
     "iopub.status.busy": "2023-10-30T01:12:30.479990Z",
     "iopub.status.idle": "2023-10-30T01:12:30.497026Z",
     "shell.execute_reply": "2023-10-30T01:12:30.496346Z"
    },
    "papermill": {
     "duration": 0.023359,
     "end_time": "2023-10-30T01:12:30.498889",
     "exception": false,
     "start_time": "2023-10-30T01:12:30.475530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/input/neurips-2023-machine-unlearning/empty.txt'):\n",
    "    # mock submission\n",
    "    subprocess.run('touch submission.zip', shell=True)\n",
    "else:\n",
    "    os.makedirs('/kaggle/tmp', exist_ok=True)\n",
    "     \n",
    "    '''\n",
    "    Get data loaders\n",
    "    '''\n",
    "    batch_size = 64\n",
    "    retain_loader, forget_loader, validation_loader = get_dataset(batch_size)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Contrastive\n",
    "    '''\n",
    "    \n",
    "    net = resnet18(weights=None, num_classes=10)\n",
    "    net.to(DEVICE)\n",
    "    net.load_state_dict(torch.load('/kaggle/input/neurips-2023-machine-unlearning/original_model.pth'))\n",
    "    \n",
    "    embeddings_retain_df, embeddings_val_df = get_embeddings(net, retain_loader, validation_loader)\n",
    "\n",
    "    # Pre-group embeddings by unique_id for fast lookup\n",
    "    grouped_val_df = embeddings_val_df.groupby('unique_id').apply(lambda x: x.iloc[:, 1:].values)\n",
    "    grouped_retain_df = embeddings_retain_df.groupby('unique_id').apply(lambda x: x.iloc[:, 1:].values)\n",
    "\n",
    "\n",
    "    '''\n",
    "    Loop\n",
    "    '''\n",
    "    \n",
    "    for i in range(512):\n",
    "        net = resnet18(weights=None, num_classes=10)\n",
    "        net.to(DEVICE)\n",
    "        net.load_state_dict(torch.load('/kaggle/input/neurips-2023-machine-unlearning/original_model.pth'))\n",
    "        for _ in range(2):\n",
    "            contrastive_unlearning(net, forget_loader, grouped_retain_df, grouped_val_df, LR=1e-4, max_num_steps=10)\n",
    "            retrain_step(net, retain_loader, LR=1e-5, max_num_steps=2)\n",
    "        state = net.state_dict()\n",
    "        torch.save(state, f'/kaggle/tmp/unlearned_checkpoint_{i}.pth')\n",
    "        gc.collect()\n",
    "    \n",
    "    # In the tmp/ folder, there will be 512 checkpoints to submit + 1 for validation early stop that doesn't get zipped\n",
    "    subprocess.run('zip submission.zip /kaggle/tmp/unlearned_*.pth', shell=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.798974,
   "end_time": "2023-10-30T01:12:31.723034",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-30T01:12:21.924060",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
