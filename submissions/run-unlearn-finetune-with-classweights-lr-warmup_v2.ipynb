{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-09-26T22:16:52.581220Z","iopub.status.busy":"2023-09-26T22:16:52.580804Z","iopub.status.idle":"2023-09-26T22:16:57.297895Z","shell.execute_reply":"2023-09-26T22:16:57.296718Z","shell.execute_reply.started":"2023-09-26T22:16:52.581178Z"},"trusted":true},"outputs":[],"source":["import os\n","import subprocess\n","import math\n","\n","import pandas as pd\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision.models import resnet18\n","from torch.utils.data import DataLoader, Dataset\n","\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-26T22:16:57.300252Z","iopub.status.busy":"2023-09-26T22:16:57.299834Z","iopub.status.idle":"2023-09-26T22:16:57.304502Z","shell.execute_reply":"2023-09-26T22:16:57.303653Z","shell.execute_reply.started":"2023-09-26T22:16:57.300226Z"},"trusted":true},"outputs":[],"source":["# It's really important to add an accelerator to your notebook, as otherwise the submission will fail.\n","# We recommend using the P100 GPU rather than T4 as it's faster and will increase the chances of passing the time cut-off threshold.\n","\n","if DEVICE != 'cuda':\n","    raise RuntimeError('Make sure you have added an accelerator to your notebook; the submission will fail otherwise!')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-26T22:16:57.306957Z","iopub.status.busy":"2023-09-26T22:16:57.306117Z","iopub.status.idle":"2023-09-26T22:16:57.322170Z","shell.execute_reply":"2023-09-26T22:16:57.321227Z","shell.execute_reply.started":"2023-09-26T22:16:57.306921Z"},"trusted":true},"outputs":[],"source":["# Helper functions for loading the hidden dataset.\n","\n","def load_example(df_row):\n","    image = torchvision.io.read_image(df_row['image_path'])\n","    result = {\n","        'image': image,\n","        'image_id': df_row['image_id'],\n","        'age_group': df_row['age_group'],\n","        'age': df_row['age'],\n","        'person_id': df_row['person_id']\n","    }\n","    return result\n","\n","\n","class HiddenDataset(Dataset):\n","    '''The hidden dataset.'''\n","    def __init__(self, split='train'):\n","        super().__init__()\n","        self.examples = []\n","\n","        df = pd.read_csv(f'/kaggle/input/neurips-2023-machine-unlearning/{split}.csv')\n","        df['image_path'] = df['image_id'].apply(\n","            lambda x: os.path.join('/kaggle/input/neurips-2023-machine-unlearning/', 'images', x.split('-')[0], x.split('-')[1] + '.png'))\n","        df = df.sort_values(by='image_path')\n","        df.apply(lambda row: self.examples.append(load_example(row)), axis=1)\n","        if len(self.examples) == 0:\n","            raise ValueError('No examples.')\n","\n","    def __len__(self):\n","        return len(self.examples)\n","\n","    def __getitem__(self, idx):\n","        example = self.examples[idx]\n","        image = example['image']\n","        image = image.to(torch.float32)\n","        example['image'] = image\n","        return example\n","\n","\n","def get_dataset(batch_size):\n","    '''Get the dataset.'''\n","    retain_ds = HiddenDataset(split='retain')\n","    forget_ds = HiddenDataset(split='forget')\n","    val_ds = HiddenDataset(split='validation')\n","\n","    retain_loader = DataLoader(retain_ds, batch_size=batch_size, shuffle=True)\n","    forget_loader = DataLoader(forget_ds, batch_size=batch_size, shuffle=True)\n","    validation_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n","\n","    return retain_loader, forget_loader, validation_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-26T22:16:57.327010Z","iopub.status.busy":"2023-09-26T22:16:57.326289Z","iopub.status.idle":"2023-09-26T22:16:57.336260Z","shell.execute_reply":"2023-09-26T22:16:57.335542Z","shell.execute_reply.started":"2023-09-26T22:16:57.326985Z"},"trusted":true},"outputs":[],"source":["# Function to update learning rate\n","def adjust_learning_rate(optimizer, current_batch, total_batches, initial_lr):\n","    \"\"\"Sets the learning rate for warmup over total_batches\"\"\"\n","    lr = initial_lr * (current_batch / total_batches)\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-26T22:16:57.338027Z","iopub.status.busy":"2023-09-26T22:16:57.337371Z","iopub.status.idle":"2023-09-26T22:16:57.350371Z","shell.execute_reply":"2023-09-26T22:16:57.349405Z","shell.execute_reply.started":"2023-09-26T22:16:57.337995Z"},"trusted":true},"outputs":[],"source":["def unlearning(\n","    net, \n","    retain_loader, \n","    forget_loader, \n","    val_loader,\n","    class_weights=None,\n","):\n","\n","    total_samples = len(retain_loader.dataset)\n","    batch_size = retain_loader.batch_size\n","    \n","    epochs = 2\n","    batches_per_epoch  = math.ceil(total_samples / batch_size)\n","    total_batches = epochs * batches_per_epoch\n","    initial_lr = 0.01\n","    warmup_batches = math.ceil(0.4*total_batches)\n","    \n","    current_batch = 0\n","    \n","    criterion = nn.CrossEntropyLoss(weight=class_weights)\n","    optimizer = optim.SGD(\n","        net.parameters(),\n","        lr=0.01,\n","        momentum=0.90,\n","        weight_decay=0,\n","    )\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n","        optimizer, T_max=epochs\n","    )\n","    net.train()\n","\n","    for ep in range(epochs):\n","\n","        for sample in retain_loader:\n","            inputs = sample[\"image\"]\n","            targets = sample[\"age_group\"]\n","            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n","            \n","            current_batch += 1\n","            \n","            # Warm-up for the first 'warmup_batches' batches\n","            if current_batch <= warmup_batches:\n","                adjust_learning_rate(optimizer, current_batch, warmup_batches, initial_lr)\n","\n","            optimizer.zero_grad()\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","            loss.backward()\n","            optimizer.step()\n","        scheduler.step()\n","\n","    net.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-26T22:16:57.353281Z","iopub.status.busy":"2023-09-26T22:16:57.353027Z","iopub.status.idle":"2023-09-26T22:16:57.371510Z","shell.execute_reply":"2023-09-26T22:16:57.370564Z","shell.execute_reply.started":"2023-09-26T22:16:57.353258Z"},"trusted":true},"outputs":[],"source":["if os.path.exists('/kaggle/input/neurips-2023-machine-unlearning/empty.txt'):\n","    # mock submission\n","    subprocess.run('touch submission.zip', shell=True)\n","else:\n","    # Load the class weights from json file of unknown structure\n","    import json\n"," \n","    class_weights_fname = \"/kaggle/input/neurips-2023-machine-unlearning/age_class_weights.json\"\n","    with open(class_weights_fname) as f:\n","        # Returns JSON object as a dictionary\n","        class_weights_dict = json.load(f)\n","\n","    # The keys should be the age_group IDs, mapping to the number of occurences for that age group.\n","    # But keys are always strings in JSON files (there are no int keys in JSON). We can't be sure\n","    # the keys in the dict are in the correct order, so let's convert the dictionary into a list\n","    # by using the expected keys.\n","    class_weights = [class_weights_dict[str(key)] for key in range(len(class_weights_dict))]\n","    # Convert list of weights into a float32 tensor\n","    class_weights = torch.tensor(class_weights).to(DEVICE, dtype=torch.float32)\n","    # The JSON file actually contains number of occurances. To correct for imbalance, the\n","    # weighting should be the reciprocal of the count instead.\n","    class_weights = 1.0 / class_weights\n","\n","    # Note: it's really important to create the unlearned checkpoints outside of the working directory \n","    # as otherwise this notebook may fail due to running out of disk space.\n","    # The below code saves them in /kaggle/tmp to avoid that issue.\n","\n","    os.makedirs('/kaggle/tmp', exist_ok=True)\n","    retain_loader, forget_loader, validation_loader = get_dataset(64)\n","    net = resnet18(weights=None, num_classes=10)\n","    net.to(DEVICE)\n","    for i in range(512):\n","        net.load_state_dict(torch.load('/kaggle/input/neurips-2023-machine-unlearning/original_model.pth'))\n","        unlearning(net, retain_loader, forget_loader, validation_loader, class_weights=class_weights)\n","        state = net.state_dict()\n","        torch.save(state, f'/kaggle/tmp/unlearned_checkpoint_{i}.pth')\n","\n","    # Ensure that submission.zip will contain exactly 512 checkpoints \n","    # (if this is not the case, an exception will be thrown).\n","    unlearned_ckpts = os.listdir('/kaggle/tmp')\n","    if len(unlearned_ckpts) != 512:\n","        raise RuntimeError('Expected exactly 512 checkpoints. The submission will throw an exception otherwise.')\n","\n","    subprocess.run('zip submission.zip /kaggle/tmp/*.pth', shell=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
