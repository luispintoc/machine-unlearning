{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport math\nimport subprocess\n\nimport pandas as pd\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.models import resnet18\nfrom torch.utils.data import DataLoader, Dataset\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# It's really important to add an accelerator to your notebook, as otherwise the submission will fail.\n# We recomment using the P100 GPU rather than T4 as it's faster and will increase the chances of passing the time cut-off threshold.\n\nif DEVICE != 'cuda':\n    raise RuntimeError('Make sure you have added an accelerator to your notebook; the submission will fail otherwise!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper functions for loading the hidden dataset.\n\ndef load_example(df_row):\n    image = torchvision.io.read_image(df_row['image_path'])\n    result = {\n        'image': image,\n        'image_id': df_row['image_id'],\n        'age_group': df_row['age_group'],\n        'age': df_row['age'],\n        'person_id': df_row['person_id']\n    }\n    return result\n\n\nclass HiddenDataset(Dataset):\n    '''The hidden dataset.'''\n    def __init__(self, split='train'):\n        super().__init__()\n        self.examples = []\n\n        df = pd.read_csv(f'/kaggle/input/neurips-2023-machine-unlearning/{split}.csv')\n        df['image_path'] = df['image_id'].apply(\n            lambda x: os.path.join('/kaggle/input/neurips-2023-machine-unlearning/', 'images', x.split('-')[0], x.split('-')[1] + '.png'))\n        df = df.sort_values(by='image_path')\n        df.apply(lambda row: self.examples.append(load_example(row)), axis=1)\n        if len(self.examples) == 0:\n            raise ValueError('No examples.')\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, idx):\n        example = self.examples[idx]\n        image = example['image']\n        image = image.to(torch.float32)\n        example['image'] = image\n        return example\n\n\ndef get_dataset(batch_size):\n    '''Get the dataset.'''\n    retain_ds = HiddenDataset(split='retain')\n    forget_ds = HiddenDataset(split='forget')\n    val_ds = HiddenDataset(split='validation')\n\n    retain_loader = DataLoader(retain_ds, batch_size=batch_size, shuffle=True)\n    forget_loader = DataLoader(forget_ds, batch_size=batch_size, shuffle=True)\n    validation_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n\n    return retain_loader, forget_loader, validation_loader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to update learning rate\ndef adjust_learning_rate(optimizer, current_batch, total_batches, initial_lr):\n    \"\"\"Sets the learning rate for warmup over total_batches\"\"\"\n    lr = initial_lr * (current_batch / total_batches)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# You can replace the below simple unlearning with your own unlearning function.\n\ndef unlearning(\n    net, \n    retain_loader, \n    forget_loader, \n    val_loader):\n\n    total_samples = len(retain_loader.dataset)\n    batch_size = retain_loader.batch_size\n\n    epochs = 2\n    batches_per_epoch  = math.ceil(total_samples / batch_size)\n    total_batches = epochs * batches_per_epoch\n    initial_lr = 0.01\n    warmup_batches = math.ceil(0.4*total_batches)\n\n    current_batch = 0\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net.parameters(), lr=0.01,\n                      momentum=0.90, weight_decay=0)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer, T_max=epochs)\n    net.train()\n\n    for ep in range(epochs):\n\n        for sample in retain_loader:\n            inputs = sample[\"image\"]\n            targets = sample[\"age_group\"]\n            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n        \n            current_batch += 1\n\n            # Warm-up for the first 'warmup_batches' batches\n            if current_batch <= warmup_batches:\n                adjust_learning_rate(optimizer, current_batch, warmup_batches, initial_lr)\n        \n            optimizer.zero_grad()\n            outputs = net(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n        scheduler.step()\n        \n    net.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if os.path.exists('/kaggle/input/neurips-2023-machine-unlearning/empty.txt'):\n    # mock submission\n    subprocess.run('touch submission.zip', shell=True)\nelse:\n    \n    # Note: it's really important to create the unlearned checkpoints outside of the working directory \n    # as otherwise this notebook may fail due to running out of disk space.\n    # The below code saves them in /kaggle/tmp to avoid that issue.\n    \n    os.makedirs('/kaggle/tmp', exist_ok=True)\n    retain_loader, forget_loader, validation_loader = get_dataset(64)\n    net = resnet18(weights=None, num_classes=10)\n    net.to(DEVICE)\n    for i in range(512):\n        net.load_state_dict(torch.load('/kaggle/input/neurips-2023-machine-unlearning/original_model.pth'))\n        unlearning(net, retain_loader, forget_loader, validation_loader)\n        state = net.state_dict()\n        torch.save(state, f'/kaggle/tmp/unlearned_checkpoint_{i}.pth')\n        \n    # Ensure that submission.zip will contain exactly 512 checkpoints \n    # (if this is not the case, an exception will be thrown).\n    unlearned_ckpts = os.listdir('/kaggle/tmp')\n    if len(unlearned_ckpts) != 512:\n        raise RuntimeError('Expected exactly 512 checkpoints. The submission will throw an exception otherwise.')\n        \n    subprocess.run('zip submission.zip /kaggle/tmp/*.pth', shell=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}