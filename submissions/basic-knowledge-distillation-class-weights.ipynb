{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a609ceb",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-13T03:44:17.206906Z",
     "iopub.status.busy": "2023-10-13T03:44:17.206463Z",
     "iopub.status.idle": "2023-10-13T03:44:21.549039Z",
     "shell.execute_reply": "2023-10-13T03:44:21.548013Z"
    },
    "papermill": {
     "duration": 4.349109,
     "end_time": "2023-10-13T03:44:21.551615",
     "exception": false,
     "start_time": "2023-10-13T03:44:17.202506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import prune\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4602e95d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T03:44:21.564509Z",
     "iopub.status.busy": "2023-10-13T03:44:21.563621Z",
     "iopub.status.idle": "2023-10-13T03:44:21.573813Z",
     "shell.execute_reply": "2023-10-13T03:44:21.571308Z"
    },
    "papermill": {
     "duration": 0.020629,
     "end_time": "2023-10-13T03:44:21.576299",
     "exception": false,
     "start_time": "2023-10-13T03:44:21.555670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# It's really important to add an accelerator to your notebook, as otherwise the submission will fail.\n",
    "# We recomment using the P100 GPU rather than T4 as it's faster and will increase the chances of passing the time cut-off threshold.\n",
    "\n",
    "if DEVICE != 'cuda':\n",
    "    raise RuntimeError('Make sure you have added an accelerator to your notebook; the submission will fail otherwise!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ddac1f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T03:44:21.588702Z",
     "iopub.status.busy": "2023-10-13T03:44:21.588243Z",
     "iopub.status.idle": "2023-10-13T03:44:21.598397Z",
     "shell.execute_reply": "2023-10-13T03:44:21.597878Z"
    },
    "papermill": {
     "duration": 0.020144,
     "end_time": "2023-10-13T03:44:21.600875",
     "exception": false,
     "start_time": "2023-10-13T03:44:21.580731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions for loading the hidden dataset.\n",
    "\n",
    "def load_example(df_row):\n",
    "    image = torchvision.io.read_image(df_row['image_path'])\n",
    "    result = {\n",
    "        'image': image,\n",
    "        'image_id': df_row['image_id'],\n",
    "        'age_group': df_row['age_group'],\n",
    "        'age': df_row['age'],\n",
    "        'person_id': df_row['person_id']\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "class HiddenDataset(Dataset):\n",
    "    '''The hidden dataset.'''\n",
    "    def __init__(self, split='train'):\n",
    "        super().__init__()\n",
    "        self.examples = []\n",
    "\n",
    "        df = pd.read_csv(f'/kaggle/input/neurips-2023-machine-unlearning/{split}.csv')\n",
    "        df['image_path'] = df['image_id'].apply(\n",
    "            lambda x: os.path.join('/kaggle/input/neurips-2023-machine-unlearning/', 'images', x.split('-')[0], x.split('-')[1] + '.png'))\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "        df.apply(lambda row: self.examples.append(load_example(row)), axis=1)\n",
    "        if len(self.examples) == 0:\n",
    "            raise ValueError('No examples.')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        image = example['image']\n",
    "        image = image.to(torch.float32)\n",
    "        example['image'] = image\n",
    "        return example\n",
    "\n",
    "\n",
    "def get_dataset(batch_size):\n",
    "    '''Get the dataset.'''\n",
    "    retain_ds = HiddenDataset(split='retain')\n",
    "    forget_ds = HiddenDataset(split='forget')\n",
    "    val_ds = HiddenDataset(split='validation')\n",
    "\n",
    "    retain_loader = DataLoader(retain_ds, batch_size=batch_size, shuffle=False)\n",
    "    forget_loader = DataLoader(forget_ds, batch_size=batch_size, shuffle=False)\n",
    "    validation_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return retain_loader, forget_loader, validation_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bbfaa46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T03:44:21.616860Z",
     "iopub.status.busy": "2023-10-13T03:44:21.616499Z",
     "iopub.status.idle": "2023-10-13T03:44:21.626234Z",
     "shell.execute_reply": "2023-10-13T03:44:21.623155Z"
    },
    "papermill": {
     "duration": 0.023577,
     "end_time": "2023-10-13T03:44:21.629046",
     "exception": false,
     "start_time": "2023-10-13T03:44:21.605469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def global_unstructure_prune(model, pruning_amount=0.2):\n",
    "\n",
    "    parameters_to_prune = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
    "            parameters_to_prune.append((module, 'weight'))\n",
    "\n",
    "    # Global pruning\n",
    "    prune.global_unstructured(\n",
    "        parameters_to_prune,\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=pruning_amount\n",
    "    )\n",
    "\n",
    "    # Make the pruning permanent\n",
    "    for module, param_name in parameters_to_prune:\n",
    "        prune.remove(module, param_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aec2a9f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T03:44:21.639805Z",
     "iopub.status.busy": "2023-10-13T03:44:21.638939Z",
     "iopub.status.idle": "2023-10-13T03:44:21.645967Z",
     "shell.execute_reply": "2023-10-13T03:44:21.645421Z"
    },
    "papermill": {
     "duration": 0.014423,
     "end_time": "2023-10-13T03:44:21.647437",
     "exception": false,
     "start_time": "2023-10-13T03:44:21.633014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_kl_loss(student_logits, teacher_logits, T=2.0, forget_flag=False):\n",
    "    \n",
    "    teacher_logits = teacher_logits/T\n",
    "\n",
    "    if forget_flag:\n",
    "        forget_T = 5.0\n",
    "        teacher_logits = teacher_logits/forget_T\n",
    "        teacher_logits = teacher_logits + 0.05*torch.rand(teacher_logits.shape).to(DEVICE)\n",
    "\n",
    "    # Calculate soft labels from teacher\n",
    "    teacher_probs = F.softmax(teacher_logits, dim=1)\n",
    "\n",
    "    # Compute distillation loss\n",
    "    student_log_probs = F.log_softmax(student_logits/T, dim=1)\n",
    "    distillation_loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean') * (T * T)\n",
    "\n",
    "    return distillation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1768d11a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T03:44:21.653355Z",
     "iopub.status.busy": "2023-10-13T03:44:21.652933Z",
     "iopub.status.idle": "2023-10-13T03:44:21.670164Z",
     "shell.execute_reply": "2023-10-13T03:44:21.669165Z"
    },
    "papermill": {
     "duration": 0.023097,
     "end_time": "2023-10-13T03:44:21.672524",
     "exception": false,
     "start_time": "2023-10-13T03:44:21.649427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to update learning rate\n",
    "def adjust_learning_rate(optimizer, current_batch, total_batches, initial_lr):\n",
    "    \"\"\"Sets the learning rate for warmup over total_batches\"\"\"\n",
    "    lr = initial_lr * (current_batch / total_batches)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09e6dbc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T03:44:21.677874Z",
     "iopub.status.busy": "2023-10-13T03:44:21.677531Z",
     "iopub.status.idle": "2023-10-13T03:44:21.687027Z",
     "shell.execute_reply": "2023-10-13T03:44:21.686113Z"
    },
    "papermill": {
     "duration": 0.014089,
     "end_time": "2023-10-13T03:44:21.688740",
     "exception": false,
     "start_time": "2023-10-13T03:44:21.674651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unlearning(\n",
    "    net, \n",
    "    retain_loader, \n",
    "    forget_loader, \n",
    "    val_loader,\n",
    "    class_weights=None,\n",
    "):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Get teacher logits\n",
    "    '''\n",
    "    \n",
    "    # Retain logits\n",
    "    teacher_retain_tensor = torch.zeros(len(retain_loader.dataset), 10)\n",
    "    start_idx = 0\n",
    "    with torch.no_grad():\n",
    "        for sample in retain_loader:\n",
    "            end_idx = start_idx + sample[\"image\"].shape[0]\n",
    "            outputs = net(sample[\"image\"].to(DEVICE))\n",
    "            teacher_retain_tensor[start_idx:end_idx] = outputs.cpu()\n",
    "            start_idx = end_idx\n",
    "\n",
    "    retain_logit_loader = DataLoader(teacher_retain_tensor, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # Forget logits\n",
    "#     teacher_forget_tensor = torch.zeros(len(forget_loader.dataset), 10)\n",
    "#     start_idx = 0\n",
    "#     with torch.no_grad():\n",
    "#         for sample in forget_loader:\n",
    "#             end_idx = start_idx + sample[\"image\"].shape[0]\n",
    "#             outputs = net(sample[\"image\"].to(DEVICE))\n",
    "#             teacher_forget_tensor[start_idx:end_idx] = outputs.cpu()\n",
    "#             start_idx = end_idx\n",
    "\n",
    "#     forget_logit_loader = DataLoader(teacher_forget_tensor, batch_size=64, shuffle=False)\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Get student\n",
    "    '''\n",
    "    \n",
    "    # Apply pruning\n",
    "    pct = 0.95\n",
    "    global_unstructure_prune(net, pct)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Training parameters\n",
    "    '''\n",
    "    \n",
    "    T = 2.0\n",
    "    alpha = 0.9\n",
    "    epochs = 1\n",
    "    \n",
    "    initial_lr = 0.001/2\n",
    "    total_samples = len(retain_loader.dataset)\n",
    "    batch_size = retain_loader.batch_size\n",
    "    batches_per_epoch  = math.ceil(total_samples / batch_size)\n",
    "    total_batches = epochs * batches_per_epoch\n",
    "    warmup_batches = math.ceil(0.2*batches_per_epoch)\n",
    "    current_batch = 0\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=initial_lr, momentum=0.90, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Training loop\n",
    "    '''\n",
    "    \n",
    "    net.train()\n",
    "\n",
    "    for ep in range(epochs):\n",
    "\n",
    "        for sample, teacher_logits in zip(retain_loader, retain_logit_loader):\n",
    "            inputs = sample[\"image\"]\n",
    "            targets = sample[\"age_group\"]\n",
    "            \n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            teacher_logits = teacher_logits.to(DEVICE)\n",
    "        \n",
    "            current_batch += 1\n",
    "\n",
    "            # Warm-up for the first 'warmup_batches' batches\n",
    "            if current_batch <= warmup_batches:\n",
    "                adjust_learning_rate(optimizer, current_batch, warmup_batches, initial_lr)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # Forward pass student\n",
    "            student_logits = net(inputs)\n",
    "            \n",
    "            # Calculate losses\n",
    "            distillation_loss = calculate_kl_loss(student_logits, teacher_logits, T=T, forget_flag=False)            \n",
    "            classification_loss = criterion(student_logits, targets)\n",
    "            loss = alpha*distillation_loss + (1-alpha)*classification_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "865dc2ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T03:44:21.693927Z",
     "iopub.status.busy": "2023-10-13T03:44:21.693583Z",
     "iopub.status.idle": "2023-10-13T03:44:21.709601Z",
     "shell.execute_reply": "2023-10-13T03:44:21.708131Z"
    },
    "papermill": {
     "duration": 0.022505,
     "end_time": "2023-10-13T03:44:21.713252",
     "exception": false,
     "start_time": "2023-10-13T03:44:21.690747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/input/neurips-2023-machine-unlearning/empty.txt'):\n",
    "    # mock submission\n",
    "    subprocess.run('touch submission.zip', shell=True)\n",
    "else:\n",
    "    # Load the class weights from json file of unknown structure\n",
    "    import json\n",
    " \n",
    "    class_weights_fname = \"/kaggle/input/neurips-2023-machine-unlearning/age_class_weights.json\"\n",
    "    with open(class_weights_fname) as f:\n",
    "        # Returns JSON object as a dictionary\n",
    "        class_weights_dict = json.load(f)\n",
    "\n",
    "    # The keys should be the age_group IDs, mapping to the number of occurences for that age group.\n",
    "    # But keys are always strings in JSON files (there are no int keys in JSON). We can't be sure\n",
    "    # the keys in the dict are in the correct order, so let's convert the dictionary into a list\n",
    "    # by using the expected keys.\n",
    "    class_weights = [class_weights_dict[str(key)] for key in range(len(class_weights_dict))]\n",
    "    # Convert list of weights into a float32 tensor\n",
    "    class_weights = torch.tensor(class_weights).to(DEVICE, dtype=torch.float32)\n",
    "    # The JSON file actually contains number of occurances. To correct for imbalance, the\n",
    "    # weighting should be the reciprocal of the count instead.\n",
    "    class_weights = 1.0 / class_weights\n",
    "\n",
    "    # Note: it's really important to create the unlearned checkpoints outside of the working directory \n",
    "    # as otherwise this notebook may fail due to running out of disk space.\n",
    "    # The below code saves them in /kaggle/tmp to avoid that issue.\n",
    "    \n",
    "    os.makedirs('/kaggle/tmp', exist_ok=True)\n",
    "\n",
    "    for i in range(512):\n",
    "        retain_loader, forget_loader, validation_loader = get_dataset(64)\n",
    "        net = resnet18(weights=None, num_classes=10)\n",
    "        net.to(DEVICE)\n",
    "        net.load_state_dict(torch.load('/kaggle/input/neurips-2023-machine-unlearning/original_model.pth'))\n",
    "        unlearning(net, retain_loader, forget_loader, validation_loader, class_weights=class_weights)\n",
    "        del retain_loader\n",
    "        del forget_loader\n",
    "        del validation_loader\n",
    "        gc.collect()\n",
    "        state = net.state_dict()\n",
    "        torch.save(state, f'/kaggle/tmp/unlearned_checkpoint_{i}.pth')\n",
    "        \n",
    "    # Ensure that submission.zip will contain exactly 512 checkpoints \n",
    "    # (if this is not the case, an exception will be thrown).\n",
    "    unlearned_ckpts = os.listdir('/kaggle/tmp')\n",
    "    if len(unlearned_ckpts) != 512:\n",
    "        raise RuntimeError('Expected exactly 512 checkpoints. The submission will throw an exception otherwise.')\n",
    "        \n",
    "    subprocess.run('zip submission.zip /kaggle/tmp/*.pth', shell=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.735497,
   "end_time": "2023-10-13T03:44:22.941526",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-13T03:44:14.206029",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
