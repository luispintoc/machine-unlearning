{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-09-25T18:40:51.347894Z","iopub.status.busy":"2023-09-25T18:40:51.346848Z","iopub.status.idle":"2023-09-25T18:40:57.103246Z","shell.execute_reply":"2023-09-25T18:40:57.101907Z","shell.execute_reply.started":"2023-09-25T18:40:51.347851Z"},"trusted":true},"outputs":[],"source":["import os\n","import subprocess\n","\n","import pandas as pd\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision.models import resnet18\n","from torch.utils.data import DataLoader, Dataset\n","\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T18:40:57.112691Z","iopub.status.busy":"2023-09-25T18:40:57.109640Z","iopub.status.idle":"2023-09-25T18:40:57.120885Z","shell.execute_reply":"2023-09-25T18:40:57.119629Z","shell.execute_reply.started":"2023-09-25T18:40:57.112650Z"},"trusted":true},"outputs":[],"source":["# It's really important to add an accelerator to your notebook, as otherwise the submission will fail.\n","# We recomment using the P100 GPU rather than T4 as it's faster and will increase the chances of passing the time cut-off threshold.\n","\n","if DEVICE != 'cuda':\n","    raise RuntimeError('Make sure you have added an accelerator to your notebook; the submission will fail otherwise!')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T18:40:57.128091Z","iopub.status.busy":"2023-09-25T18:40:57.125866Z","iopub.status.idle":"2023-09-25T18:40:57.151537Z","shell.execute_reply":"2023-09-25T18:40:57.150200Z","shell.execute_reply.started":"2023-09-25T18:40:57.128022Z"},"trusted":true},"outputs":[],"source":["# Helper functions for loading the hidden dataset.\n","\n","def load_example(df_row):\n","    image = torchvision.io.read_image(df_row['image_path'])\n","    result = {\n","        'image': image,\n","        'image_id': df_row['image_id'],\n","        'age_group': df_row['age_group'],\n","        'age': df_row['age'],\n","        'person_id': df_row['person_id']\n","    }\n","    return result\n","\n","\n","class HiddenDataset(Dataset):\n","    '''The hidden dataset.'''\n","    def __init__(self, split='train'):\n","        super().__init__()\n","        self.examples = []\n","\n","        df = pd.read_csv(f'/kaggle/input/neurips-2023-machine-unlearning/{split}.csv')\n","        df['image_path'] = df['image_id'].apply(\n","            lambda x: os.path.join('/kaggle/input/neurips-2023-machine-unlearning/', 'images', x.split('-')[0], x.split('-')[1] + '.png'))\n","        df = df.sort_values(by='image_path')\n","        df.apply(lambda row: self.examples.append(load_example(row)), axis=1)\n","        if len(self.examples) == 0:\n","            raise ValueError('No examples.')\n","\n","    def __len__(self):\n","        return len(self.examples)\n","\n","    def __getitem__(self, idx):\n","        example = self.examples[idx]\n","        image = example['image']\n","        image = image.to(torch.float32)\n","        example['image'] = image\n","        return example\n","\n","\n","def get_dataset(batch_size):\n","    '''Get the dataset.'''\n","    retain_ds = HiddenDataset(split='retain')\n","    forget_ds = HiddenDataset(split='forget')\n","    val_ds = HiddenDataset(split='validation')\n","\n","    retain_loader = DataLoader(retain_ds, batch_size=batch_size, shuffle=True)\n","    forget_loader = DataLoader(forget_ds, batch_size=batch_size, shuffle=True)\n","    validation_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n","\n","    return retain_loader, forget_loader, validation_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T18:40:57.156093Z","iopub.status.busy":"2023-09-25T18:40:57.155139Z","iopub.status.idle":"2023-09-25T18:40:57.166144Z","shell.execute_reply":"2023-09-25T18:40:57.164935Z","shell.execute_reply.started":"2023-09-25T18:40:57.156023Z"},"trusted":true},"outputs":[],"source":["# def accuracy(net, loader):\n","#     \"\"\"Return accuracy on a dataset given by the data loader.\"\"\"\n","#     correct = 0\n","#     total = 0\n","#     for sample in loader:\n","#         inputs = sample[\"image\"]\n","#         targets = sample[\"age_group\"]\n","#         inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n","#         outputs = net(inputs)\n","#         _, predicted = outputs.max(1)\n","#         total += targets.size(0)\n","#         correct += predicted.eq(targets).sum().item()\n","#     return correct / total"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T18:40:57.172363Z","iopub.status.busy":"2023-09-25T18:40:57.171310Z","iopub.status.idle":"2023-09-25T18:40:57.184381Z","shell.execute_reply":"2023-09-25T18:40:57.183213Z","shell.execute_reply.started":"2023-09-25T18:40:57.172319Z"},"trusted":true},"outputs":[],"source":["# def combine_loaders_randomly(loader1, loader2, net):\n","#     \"\"\"Combine two PyTorch DataLoader objects into a single generator, \n","#        randomly picking from each and modifying targets from loader2.\"\"\"\n","    \n","#     iter1 = iter(loader1)\n","#     iter2 = iter(loader2)\n","    \n","#     while True:\n","#         chosen_loader = random.choices(['loader1', 'loader2'], weights=[90, 10], k=1)[0]\n","        \n","#         if chosen_loader == 'loader1':\n","#             try:\n","#                 batch = next(iter1)\n","#                 yield batch\n","#             except StopIteration:\n","#                 iter1 = iter(loader1)  # Reset the iterator if exhausted\n","#         else:\n","#             try:\n","#                 inputs, _ = next(iter2)  # Ignore original targets\n","#                 inputs = inputs.to(DEVICE)\n","                \n","#                 # Compute new targets\n","#                 with torch.no_grad():\n","#                     preds = net(inputs)\n","#                     _, new_targets = preds.min(dim=1)\n","                \n","#                 yield inputs, new_targets\n","#             except StopIteration:\n","#                 iter2 = iter(loader2)  # Reset the iterator if exhausted"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T18:40:57.188004Z","iopub.status.busy":"2023-09-25T18:40:57.187176Z","iopub.status.idle":"2023-09-25T18:40:57.214171Z","shell.execute_reply":"2023-09-25T18:40:57.212831Z","shell.execute_reply.started":"2023-09-25T18:40:57.187964Z"},"trusted":true},"outputs":[],"source":["# You can replace the below simple unlearning with your own unlearning function.\n","\n","def unlearning(\n","    net, \n","    retain_loader, \n","    forget_loader, \n","    val_loader):\n","    \n","    epochs = 1\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.SGD(net.parameters(), lr=0.0005,\n","                      momentum=0.9, weight_decay=0)\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n","        optimizer, T_max=epochs)\n","    net.train()\n","\n","    for ep in range(epochs):\n","        \n","        # Initialize iterators at the beginning of each epoch\n","        iter_retain = iter(retain_loader)\n","        iter_forget = iter(forget_loader)\n","\n","        # Initialize weights for random selection\n","        weights = torch.tensor([80, 20], dtype=torch.float32)\n","\n","        # Initialize flag for depleted iterator\n","        one_depleted = False\n","\n","#         while not one_depleted:\n","        for _ in range(100):\n","            # Randomly choose a loader based on weights\n","            chosen_loader_idx = torch.multinomial(weights, 1).item()\n","\n","            if chosen_loader_idx == 0:  # corresponds to retain_loader\n","                try:\n","                    batch = next(iter_retain)\n","                except StopIteration:\n","                    one_depleted = True\n","                    continue  # Skip to the next iteration\n","\n","            else:  # corresponds to forget_loader\n","                try:\n","                    batch = next(iter_forget)\n","                except StopIteration:\n","                    one_depleted = True\n","                    continue  # Skip to the next iteration\n","\n","            sample = batch\n","            inputs = sample[\"image\"]\n","            targets = sample[\"age_group\"]\n","            if chosen_loader_idx == 1:\n","                # Compute new targets\n","                net.eval()\n","                with torch.no_grad():\n","                    preds = net(inputs.to(DEVICE))\n","                    _, targets = torch.topk(preds, 2, dim=1)\n","                    targets = targets[:, -1] # Choose worst prediction\n","            \n","            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n","            \n","            net.train()\n","            optimizer.zero_grad()\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","            loss.backward()\n","            optimizer.step()\n","        scheduler.step()\n","        \n","    net.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T18:40:57.223339Z","iopub.status.busy":"2023-09-25T18:40:57.221811Z","iopub.status.idle":"2023-09-25T18:40:57.233339Z","shell.execute_reply":"2023-09-25T18:40:57.232215Z","shell.execute_reply.started":"2023-09-25T18:40:57.223294Z"},"trusted":true},"outputs":[],"source":["# def unlearning(net, retain, forget, validation):\n","#     \"\"\"Unlearning by fine-tuning.\n","\n","#     Fine-tuning is a very simple algorithm that trains using only\n","#     the retain set.\n","\n","#     Args:\n","#       net : nn.Module.\n","#         pre-trained model to use as base of unlearning.\n","#       retain : torch.utils.data.DataLoader.\n","#         Dataset loader for access to the retain set. This is the subset\n","#         of the training set that we don't want to forget.\n","#       forget : torch.utils.data.DataLoader.\n","#         Dataset loader for access to the forget set. This is the subset\n","#         of the training set that we want to forget. This method doesn't\n","#         make use of the forget set.\n","#       validation : torch.utils.data.DataLoader.\n","#         Dataset loader for access to the validation set. This method doesn't\n","#         make use of the validation set.\n","#     Returns:\n","#       net : updated model\n","#     \"\"\"\n","#     epochs = 1\n","#     early_stop_steps = 5  # Number of steps to consider for early stopping\n","#     early_stop_threshold = 0.05  # 5% validation performance decrease\n","\n","#     criterion = nn.CrossEntropyLoss()\n","#     optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n","#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n","\n","#     # Record the original performance on the validation set\n","#     original_val_accuracy = accuracy(net, validation)\n","    \n","#     net.train()\n","#     steps_without_improvement = 0\n","#     step_count = 0\n","\n","#     combined_loader = combine_loaders_randomly(retain, forget, net)\n","\n","#     for _ in range(epochs):\n","#         for sample in combined_loader:\n","#             inputs = sample[\"image\"]\n","#             targets = sample[\"age_group\"]\n","#             inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n","#             optimizer.zero_grad()\n","#             outputs = net(inputs)\n","#             loss = criterion(outputs, targets)\n","#             loss.backward()\n","#             optimizer.step()\n","\n","#             step_count += 1\n","\n","#             # Evaluate on validation set\n","#             current_val_accuracy = accuracy(net, validation)\n","# #             print(f\"Validation acc {current_val_accuracy:.2f}\")\n","\n","#             # Check for performance decrease\n","#             if current_val_accuracy < (1 - early_stop_threshold) * original_val_accuracy:\n","#                 steps_without_improvement += 1\n","#                 if steps_without_improvement >= early_stop_steps:\n","# #                     print(f\"Early stopping triggered at step {step_count}.\")\n","#                     net.eval()\n","#                     return net\n","#             else:\n","#                 steps_without_improvement = 0\n","\n","\n","#         scheduler.step()\n","\n","#     net.eval()\n","#     return net"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T18:40:57.242434Z","iopub.status.busy":"2023-09-25T18:40:57.239165Z","iopub.status.idle":"2023-09-25T18:40:57.264297Z","shell.execute_reply":"2023-09-25T18:40:57.263075Z","shell.execute_reply.started":"2023-09-25T18:40:57.242289Z"},"trusted":true},"outputs":[],"source":["if os.path.exists('/kaggle/input/neurips-2023-machine-unlearning/empty.txt'):\n","    # mock submission\n","    subprocess.run('touch submission.zip', shell=True)\n","else:\n","    \n","    # Note: it's really important to create the unlearned checkpoints outside of the working directory \n","    # as otherwise this notebook may fail due to running out of disk space.\n","    # The below code saves them in /kaggle/tmp to avoid that issue.\n","    \n","    os.makedirs('/kaggle/tmp', exist_ok=True)\n","    retain_loader, forget_loader, validation_loader = get_dataset(64)\n","    net = resnet18(weights=None, num_classes=10)\n","    net.to(DEVICE)\n","    for i in range(512):\n","        net.load_state_dict(torch.load('/kaggle/input/neurips-2023-machine-unlearning/original_model.pth'))\n","        unlearning(net, retain_loader, forget_loader, validation_loader)\n","        state = net.state_dict()\n","        torch.save(state, f'/kaggle/tmp/unlearned_checkpoint_{i}.pth')\n","        \n","    # Ensure that submission.zip will contain exactly 512 checkpoints \n","    # (if this is not the case, an exception will be thrown).\n","    unlearned_ckpts = os.listdir('/kaggle/tmp')\n","    if len(unlearned_ckpts) != 512:\n","        raise RuntimeError('Expected exactly 512 checkpoints. The submission will throw an exception otherwise.')\n","        \n","    subprocess.run('zip submission.zip /kaggle/tmp/*.pth', shell=True)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
