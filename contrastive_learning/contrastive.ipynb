{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: CUDA\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, model_selection\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn.utils import prune\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Running on device:\", DEVICE.upper())\n",
    "\n",
    "# manual random seed is used for dataset partitioning\n",
    "# to ensure reproducible results across runs\n",
    "SEED = 42\n",
    "RNG = torch.Generator().manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "# Create an unverified SSL context\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and pre-process CIFAR10\n",
    "normalize = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(\n",
    "    root=\"../example notebooks/data\", train=True, download=False, transform=normalize\n",
    ")\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "# we split held out data into test and validation set\n",
    "held_out = torchvision.datasets.CIFAR10(\n",
    "    root=\"../example notebooks/data\", train=False, download=False, transform=normalize\n",
    ")\n",
    "test_set, val_set = torch.utils.data.random_split(held_out, [0.5, 0.5], generator=RNG)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "# download the forget and retain index split\n",
    "local_path = \"../example notebooks/forget_idx.npy\"\n",
    "# if not os.path.exists(local_path):\n",
    "#     response = requests.get(\n",
    "#         \"https://storage.googleapis.com/unlearning-challenge/\" + local_path\n",
    "#     )\n",
    "#     open(local_path, \"wb\").write(response.content)\n",
    "forget_idx = np.load(local_path)\n",
    "\n",
    "# construct indices of retain from those of the forget set\n",
    "forget_mask = np.zeros(len(train_set.targets), dtype=bool)\n",
    "forget_mask[forget_idx] = True\n",
    "retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
    "\n",
    "# split train set into a forget and a retain set\n",
    "forget_set = torch.utils.data.Subset(train_set, forget_idx)\n",
    "retain_set = torch.utils.data.Subset(train_set, retain_idx)\n",
    "\n",
    "forget_loader = torch.utils.data.DataLoader(\n",
    "    forget_set, batch_size=batch_size, shuffle=True, num_workers=1\n",
    ")\n",
    "# retain_loader = torch.utils.data.DataLoader(\n",
    "#     retain_set, batch_size=128, shuffle=True, num_workers=1, generator=RNG\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "retain_loader = torch.utils.data.DataLoader(\n",
    "    retain_set, batch_size=batch_size, shuffle=True, num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Retain set accuracy: 99.5%\n",
    "- Forget set accuracy: 99.3%\n",
    "- Val set accuracy: 88.9%\n",
    "- Test set accuracy: 88.3%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"../example notebooks/weights/weights_resnet18_cifar10.pth\"\n",
    "if not os.path.exists(local_path):\n",
    "    response = requests.get(\n",
    "        \"https://storage.googleapis.com/unlearning-challenge/weights_resnet18_cifar10.pth\"\n",
    "    )\n",
    "    open(local_path, \"wb\").write(response.content)\n",
    "\n",
    "weights_pretrained = torch.load(local_path, map_location=DEVICE) #43Mbs\n",
    "\n",
    "# load net with pre-trained weights\n",
    "net = resnet18(weights=None, num_classes=10)\n",
    "net.load_state_dict(weights_pretrained)\n",
    "net.to(DEVICE)\n",
    "net.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_losses = compute_losses(net, val_loader)\n",
    "# test_losses = compute_losses(net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature and pooling layers to create a Custom Model\n",
    "class CustomResNet18(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(CustomResNet18, self).__init__()\n",
    "        \n",
    "        # Extract features and pooling layers\n",
    "        self.features = nn.Sequential(*list(original_model.children())[:-2])\n",
    "        self.pooling = list(original_model.children())[-2]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pooling(x)\n",
    "        x = torch.squeeze(x)\n",
    "        return x\n",
    "\n",
    "custom_model = CustomResNet18(net).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "val_embeddings = {}\n",
    "retain_embeddings = {}\n",
    "\n",
    "# Compute embeddings for val_loader\n",
    "for val_batch in val_loader:\n",
    "    images = val_batch[0].to(DEVICE)\n",
    "    person_ids = val_batch[1]\n",
    "    embeddings = custom_model(images)\n",
    "    for i, person_id in enumerate(person_ids):\n",
    "        val_embeddings.setdefault(person_id.item(), []).append(embeddings[i].detach())\n",
    "\n",
    "# Compute embeddings for retain_loader\n",
    "for retain_batch in retain_loader:\n",
    "    images = retain_batch[0].to(DEVICE)\n",
    "    targets = retain_batch[1]\n",
    "    embeddings = net(images)\n",
    "    for i, target in enumerate(targets):\n",
    "        if target in range(0,10): # TODO [0, 1]:  # Only consider targets 0 and 1\n",
    "            retain_embeddings.setdefault(target.item(), []).append(embeddings[i].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luizi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3406: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Contrastive Loss\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = nn.functional.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) + (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive\n",
    "\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.AdamW(custom_model.parameters(), lr=0.001)\n",
    "\n",
    "# Step 3: Contrastive Learning for one epoch\n",
    "for batch in forget_loader:\n",
    "    custom_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    inputs = batch[0].to(DEVICE)\n",
    "    targets = batch[1]\n",
    "    person_ids = batch[1]\n",
    "    \n",
    "    # Forward pass to get embeddings for the forget_batch\n",
    "    forget_embeddings = custom_model(inputs)\n",
    "    \n",
    "    positive_pairs = []\n",
    "    negative_pairs = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation to save memory\n",
    "        # Fetch Positive Pairs\n",
    "        for pid in person_ids:\n",
    "            pid = pid.item()\n",
    "            if pid in val_embeddings:\n",
    "                selected_embedding = random.choice(val_embeddings[pid])\n",
    "                positive_pairs.append(selected_embedding)\n",
    "            else:\n",
    "                print(f\"Skipping person_id {pid} for positive pairs, not found in val_embeddings.\")\n",
    "                continue\n",
    "\n",
    "        # Convert to tensors for ease of computation\n",
    "        positive_pairs = torch.stack(positive_pairs).to(DEVICE)\n",
    "\n",
    "        # Fetch Negative Pairs\n",
    "        for tgt in targets:\n",
    "            tgt = tgt.item()\n",
    "            if tgt in retain_embeddings:\n",
    "                selected_embedding = random.choice(retain_embeddings[tgt])\n",
    "                negative_pairs.append(selected_embedding)\n",
    "            else:\n",
    "                print(f\"Skipping target {tgt} for negative pairs, not found in retain_embeddings.\")\n",
    "                continue\n",
    "\n",
    "        # Convert to tensors for ease of computation\n",
    "        # negative_pairs = torch.stack(negative_pairs).to(DEVICE)\n",
    "\n",
    "    import sys\n",
    "    sys.exit()\n",
    "    \n",
    "    # Compute Contrastive Loss\n",
    "    positive_loss = criterion(forget_embeddings, positive_pairs, torch.zeros(positive_pairs.shape[0]).to(DEVICE))\n",
    "    negative_loss = criterion(forget_embeddings, negative_pairs, torch.ones(negative_pairs.shape[0]).to(DEVICE))\n",
    "    \n",
    "    # Total loss\n",
    "    loss = positive_loss + negative_loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        forget_acc = accuracy(net, forget_loader)\n",
    "    print(f\"Forget set accuracy: {100.0 * forget_acc:0.2f}%\")\n",
    "    print('--'*10)\n",
    "\n",
    "    if forget_acc < 0.86:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output1 shape:  torch.Size([32, 512])\n",
      "output2 shape:  torch.Size([32, 10])\n",
      "label shape:  torch.Size([32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (512) must match the size of tensor b (10) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\luizi\\OneDrive\\Documentos\\GitHub\\machine-unlearning\\contrastive_learning\\contrastive.ipynb Cell 12\u001b[0m line \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/luizi/OneDrive/Documentos/GitHub/machine-unlearning/contrastive_learning/contrastive.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39moutput2 shape: \u001b[39m\u001b[39m\"\u001b[39m, negative_pairs\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/luizi/OneDrive/Documentos/GitHub/machine-unlearning/contrastive_learning/contrastive.ipynb#X44sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlabel shape: \u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39mones(negative_pairs\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mto(DEVICE)\u001b[39m.\u001b[39mshape)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/luizi/OneDrive/Documentos/GitHub/machine-unlearning/contrastive_learning/contrastive.ipynb#X44sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m euclidean_distance \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mpairwise_distance(forget_embeddings, negative_pairs)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (10) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"output1 shape: \", forget_embeddings.shape)\n",
    "print(\"output2 shape: \", negative_pairs.shape)\n",
    "print(\"label shape: \", torch.ones(negative_pairs.shape[0]).to(DEVICE).shape)\n",
    "euclidean_distance = nn.functional.pairwise_distance(forget_embeddings, negative_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20.1439, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(forget_embeddings, positive_pairs, torch.zeros(positive_pairs.shape[0]).to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_pairs = []\n",
    "\n",
    "# Fetch Negative Pairs\n",
    "for tgt in targets:\n",
    "    tgt = tgt.item()\n",
    "    if tgt in retain_embeddings:\n",
    "        print('a')\n",
    "        selected_embedding = random.choice(retain_embeddings[tgt])\n",
    "        negative_pairs.append(selected_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_pairs = torch.stack(negative_pairs).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8050e+00, -1.8805e+00, -1.4124e+00, -1.8482e-01, -2.2213e-01,\n",
       "          9.9724e-01, -1.0107e+00,  9.7968e+00, -2.1353e+00, -2.1434e+00],\n",
       "        [-4.2494e-02,  5.8165e+00, -2.1119e-01, -2.0664e+00, -3.3988e+00,\n",
       "         -1.3326e+00, -1.6098e+00,  1.3352e-01, -1.8334e+00,  4.5444e+00],\n",
       "        [-1.0446e+00, -2.5142e+00, -3.9545e+00,  1.0687e+01, -8.3356e-01,\n",
       "          4.8609e+00, -1.7591e+00, -1.5781e+00, -1.9732e+00, -1.8909e+00],\n",
       "        [ 8.5357e+00,  1.5755e+00, -6.3768e-01, -2.6231e+00, -1.8164e+00,\n",
       "         -1.8184e+00, -1.6429e+00, -1.8333e+00,  1.0776e+00, -8.1713e-01],\n",
       "        [-6.0232e-01, -2.1849e+00, -1.7728e+00, -1.9746e+00,  1.0377e+01,\n",
       "         -3.5582e+00, -9.5972e-01,  1.2621e+00, -7.8732e-01,  2.0070e-01],\n",
       "        [-2.8061e+00, -2.1484e+00, -1.5990e+00,  7.1116e-01,  4.6521e+00,\n",
       "          9.5043e+00, -1.1965e+00, -7.6086e-01, -2.8754e+00, -3.4815e+00],\n",
       "        [ 8.3634e-01, -1.0836e+00,  1.1408e+00, -9.6132e-01, -3.2298e+00,\n",
       "         -3.0115e+00,  7.7856e+00, -2.2155e+00, -2.1495e-01,  9.5373e-01],\n",
       "        [-2.9714e-01,  1.0456e+00, -7.9674e-01, -2.4870e+00, -8.8934e-01,\n",
       "         -1.5664e+00, -2.2551e+00,  1.2779e+00, -2.1923e+00,  8.1603e+00],\n",
       "        [ 8.8543e+00,  1.4468e+00, -1.6296e+00, -1.7240e+00,  1.2449e-01,\n",
       "         -3.4198e+00, -2.6010e+00, -3.6394e+00,  4.2342e+00, -1.6463e+00],\n",
       "        [-2.9081e+00, -2.2556e+00,  6.7761e-01,  1.3726e-01,  3.6955e-01,\n",
       "          3.1558e-01, -2.3996e+00,  1.0600e+01, -3.2427e+00, -1.2938e+00],\n",
       "        [-1.8374e+00,  8.5187e+00, -5.1527e-01, -1.8948e+00, -1.5320e+00,\n",
       "          3.4520e-01, -1.3993e+00, -3.3634e-01, -1.3292e-01, -1.2160e+00],\n",
       "        [ 2.4391e+00,  2.5647e+00, -3.5659e+00, -9.3018e-01, -2.8634e+00,\n",
       "         -2.2622e+00, -3.4626e-01, -2.9940e+00,  9.2245e+00, -1.2666e+00],\n",
       "        [-7.2168e-01, -2.0769e+00,  7.2673e-01,  1.0282e+01, -2.0156e+00,\n",
       "          2.4056e+00, -8.6661e-01, -3.2055e+00, -2.2487e+00, -2.2800e+00],\n",
       "        [ 1.0785e-01,  7.9447e+00, -8.2777e-01, -1.4691e+00, -9.0979e-01,\n",
       "         -1.7845e+00, -7.2376e-01, -2.7568e+00, -5.5835e-01,  9.7737e-01],\n",
       "        [ 9.1145e-01,  9.9841e+00, -2.5305e+00, -1.9863e+00, -2.4152e+00,\n",
       "         -3.1669e+00, -2.3095e+00, -3.9372e+00,  2.4186e+00,  3.0313e+00],\n",
       "        [ 4.0808e+00, -3.0717e+00, -1.6756e-01, -1.9932e+00, -1.7237e+00,\n",
       "         -3.0705e+00, -1.4265e+00, -3.0141e+00,  8.0841e+00,  2.3020e+00],\n",
       "        [-3.6604e+00, -2.2290e-01,  1.6449e+00, -6.7446e-01, -2.1676e+00,\n",
       "          2.7867e+00, -2.6615e+00,  8.9978e+00, -2.1114e+00, -1.9313e+00],\n",
       "        [-1.8604e+00, -1.6590e+00, -1.3133e+00, -3.9774e-01, -1.7095e-01,\n",
       "          1.8565e+00, -8.5594e-01,  8.4310e+00, -1.6809e+00, -2.3493e+00],\n",
       "        [-1.7031e+00,  1.0240e-01, -2.9009e+00,  1.5032e+00, -6.9868e-01,\n",
       "         -2.3974e+00,  9.7070e+00, -1.1423e+00, -2.1659e+00, -3.0462e-01],\n",
       "        [-1.4319e+00, -1.4683e+00,  3.2597e+00,  2.0532e-01,  3.3947e-01,\n",
       "         -3.2323e+00,  8.7373e+00, -2.8236e+00, -1.4301e+00, -2.1557e+00],\n",
       "        [ 1.1946e+01, -2.7953e+00, -1.3056e+00, -3.7382e-01, -1.5457e+00,\n",
       "         -2.2341e+00, -2.4257e+00, -1.6799e+00,  6.1682e-01, -2.0310e-01],\n",
       "        [-1.2601e+00, -2.1869e+00,  1.1427e+00, -2.2234e+00,  8.6868e+00,\n",
       "         -1.1227e+00, -2.7602e+00,  2.8179e+00, -8.4967e-01, -2.2446e+00],\n",
       "        [ 1.0495e+01, -6.2540e-01, -2.3383e+00, -2.9555e-01, -7.9204e-01,\n",
       "         -1.6749e+00, -2.5895e+00, -2.3522e+00,  6.7223e-01, -5.0015e-01],\n",
       "        [-1.4969e+00, -2.2674e+00, -5.6172e-01,  1.0949e-01,  2.2271e-01,\n",
       "         -1.3664e+00, -7.7598e-01,  9.7078e+00, -2.3090e+00, -1.2628e+00],\n",
       "        [-1.4266e+00, -7.0051e-01, -9.7551e-02,  6.4131e-03, -1.7458e+00,\n",
       "         -1.7447e+00, -8.8048e-01, -2.2211e+00,  9.6858e+00, -8.7572e-01],\n",
       "        [-8.3272e-01, -8.4381e-01, -9.7767e-01, -9.2970e-01, -2.4528e-01,\n",
       "         -1.3803e+00, -4.9649e-01, -1.2224e+00, -1.5724e+00,  8.5005e+00],\n",
       "        [ 1.1040e+00, -2.9206e+00,  9.1506e+00, -1.0999e+00,  2.6102e+00,\n",
       "         -3.3140e+00, -5.6376e-01,  1.0516e+00, -3.0333e+00, -2.9851e+00],\n",
       "        [-1.1481e+00,  1.0366e+01, -1.8554e+00, -2.2873e+00, -2.4862e+00,\n",
       "         -1.6919e+00, -6.8799e-01, -1.7982e+00, -9.1222e-03,  1.5984e+00],\n",
       "        [ 4.8426e+00, -4.6922e+00,  9.9298e+00, -1.2425e+00,  1.9618e+00,\n",
       "         -2.4832e+00, -8.4903e-01, -3.2065e+00, -1.7519e+00, -2.5090e+00],\n",
       "        [-3.1453e+00, -1.2335e-01,  1.1665e+00, -1.9780e+00,  9.9149e+00,\n",
       "         -1.2616e+00, -1.1081e+00, -1.0517e-02, -3.0350e+00, -4.1973e-01],\n",
       "        [-2.2787e+00, -2.5460e+00,  8.7909e-01, -1.3641e+00,  2.5186e+00,\n",
       "         -2.2851e+00, -6.7458e-01,  8.3916e+00,  2.2511e-03, -2.6432e+00],\n",
       "        [ 3.2270e-01, -1.6374e+00, -1.7084e+00,  4.7286e-01, -1.9189e+00,\n",
       "         -5.0016e-01,  8.9259e-02,  8.1174e+00, -1.3808e+00, -1.8569e+00]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (512) must match the size of tensor b (10) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\luizi\\OneDrive\\Documentos\\GitHub\\machine-unlearning\\contrastive_learning\\contrastive.ipynb Cell 14\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/luizi/OneDrive/Documentos/GitHub/machine-unlearning/contrastive_learning/contrastive.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m criterion(forget_embeddings, negative_pairs, torch\u001b[39m.\u001b[39;49mones(negative_pairs\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m])\u001b[39m.\u001b[39;49mto(DEVICE))\n",
      "File \u001b[1;32mc:\\Users\\luizi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\luizi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\luizi\\OneDrive\\Documentos\\GitHub\\machine-unlearning\\contrastive_learning\\contrastive.ipynb Cell 14\u001b[0m line \u001b[0;36mContrastiveLoss.forward\u001b[1;34m(self, output1, output2, label)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/luizi/OneDrive/Documentos/GitHub/machine-unlearning/contrastive_learning/contrastive.ipynb#X41sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, output1, output2, label):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/luizi/OneDrive/Documentos/GitHub/machine-unlearning/contrastive_learning/contrastive.ipynb#X41sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     euclidean_distance \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mpairwise_distance(output1, output2)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/luizi/OneDrive/Documentos/GitHub/machine-unlearning/contrastive_learning/contrastive.ipynb#X41sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     loss_contrastive \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean((\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mlabel) \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mpow(euclidean_distance, \u001b[39m2\u001b[39m) \u001b[39m+\u001b[39m (label) \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mpow(torch\u001b[39m.\u001b[39mclamp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmargin \u001b[39m-\u001b[39m euclidean_distance, \u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m), \u001b[39m2\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luizi/OneDrive/Documentos/GitHub/machine-unlearning/contrastive_learning/contrastive.ipynb#X41sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_contrastive\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (10) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "criterion(forget_embeddings, negative_pairs, torch.ones(negative_pairs.shape[0]).to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 512])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forget_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([2, 9, 6, 5, 8, 3, 4, 0, 7, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retain_embeddings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 2, 1, 7, 0, 9, 7, 8])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forget_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize custom model with original model's weights\n",
    "custom_model = CustomResNet18(net)\n",
    "\n",
    "# Enable training for layers before pooling\n",
    "for param in custom_model.features[-1].parameters():  # Assuming the last layer is layer4\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Contrastive Learning\n",
    "optimizer = optim.SGD(custom_model.features[-1].parameters(), lr=0.001)\n",
    "triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Contrastive Learning\n",
    "for sample in forget_loader:\n",
    "    optimizer.zero_grad()\n",
    "    inputs_forget = sample[0]\n",
    "    person_id_forget = sample[1]\n",
    "    \n",
    "    embeddings_forget = custom_model(inputs_forget)\n",
    "\n",
    "    # Find Positive Pairs in val_loader\n",
    "    embeddings_val = []\n",
    "    for val_sample in val_loader:\n",
    "        if val_sample[1] == person_id_forget:\n",
    "            embeddings_val.append(custom_model(val_sample[0]))\n",
    "    embeddings_val = torch.stack(embeddings_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pruning\n",
    "# pct = 0.10\n",
    "# unstructure_prune(net, pct, global_pruning=True, random_init=False)\n",
    "\n",
    "torch.save({\n",
    "    'net': net.state_dict(),\n",
    "}, f'./checkpoints/temp_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forget_step(net, forget_loader, starting_forget_acc, counter_bool=0):\n",
    "\n",
    "    # Initialize for all epochs\n",
    "    forget_acc = copy.copy(starting_forget_acc)\n",
    "    print(f'Starting with {100.0 * forget_acc:0.2f}% forget accuracy')\n",
    "    forget_acc_threshold_1 = forget_acc*0.90\n",
    "    forget_acc_threshold_2 = forget_acc*0.80\n",
    "    print(f'Thresholds: {100.0 * forget_acc_threshold_1:0.2f}%, {100.0 * forget_acc_threshold_2:0.2f}%')\n",
    "\n",
    "    \n",
    "    iter_forget = iter(forget_loader)\n",
    "    current_batch = 0\n",
    "\n",
    "    initial_forget_lr = LR\n",
    "    if counter_bool>0:\n",
    "        initial_forget_lr = initial_forget_lr*(1.5**counter_bool)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.0)\n",
    "    forget_optimizer = optim.AdamW(net.parameters(), lr=initial_forget_lr)\n",
    "\n",
    "    not_depleted = True\n",
    "    counter = 0\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    while (forget_acc > forget_acc_threshold_1) & (not_depleted):\n",
    "\n",
    "        try:\n",
    "            sample = next(iter_forget)\n",
    "            counter+=1\n",
    "            \n",
    "        except StopIteration:\n",
    "            not_depleted = False\n",
    "            print('depleted')\n",
    "            break\n",
    "\n",
    "        inputs, targets = sample\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        forget_optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = net(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        classification_loss = criterion(logits, targets)\n",
    "\n",
    "        loss = -1*classification_loss\n",
    "        loss.backward()\n",
    "        forget_optimizer.step()\n",
    "\n",
    "        current_batch+=1\n",
    "        print(current_batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            forget_acc = accuracy(net, forget_loader)\n",
    "        print(f\"Forget set accuracy: {100.0 * forget_acc:0.2f}%\")\n",
    "        print('--'*10)\n",
    "\n",
    "        if (forget_acc<forget_acc_threshold_2):\n",
    "            initial_forget_lr = initial_forget_lr/2\n",
    "            current_batch = 0\n",
    "            forget_acc = starting_forget_acc\n",
    "            print('Restoring')\n",
    "            checkpoint = torch.load(f'./checkpoints/temp_checkpoint.pth')\n",
    "            net.load_state_dict(checkpoint['net'])\n",
    "            forget_optimizer = optim.AdamW(net.parameters(), lr=initial_forget_lr)\n",
    "\n",
    "        if counter>=10:\n",
    "            break\n",
    "\n",
    "    return net, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_step(net, retain_loader):\n",
    "\n",
    "    initial_retain_lr = LR\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_retain = optim.SGD(net.parameters(), lr=initial_retain_lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    warmup_current_batch = 0\n",
    "    warmup_batches = math.ceil(0.4*len(retain_loader.dataset))\n",
    "    \n",
    "    net.train()\n",
    "\n",
    "    for sample in retain_loader:\n",
    "\n",
    "        inputs, targets = sample\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        warmup_current_batch += 1\n",
    "\n",
    "        # Warm-up for the first 'warmup_batches' batches\n",
    "        if warmup_current_batch <= warmup_batches:\n",
    "            adjust_learning_rate(optimizer_retain, warmup_current_batch, warmup_batches, initial_retain_lr)\n",
    "\n",
    "        optimizer_retain.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = net(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.4)\n",
    "        classification_loss = criterion(logits, targets)\n",
    "        loss = classification_loss\n",
    "        loss.backward()\n",
    "        optimizer_retain.step()\n",
    "\n",
    "    torch.save({\n",
    "        'net': net.state_dict(),\n",
    "    }, f'./checkpoints/temp_checkpoint.pth')\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "counter_bool = 0\n",
    "\n",
    "for ep in range(epochs):\n",
    "\n",
    "    print(f'Epoch: {ep}')\n",
    "    print('****'*10)\n",
    "\n",
    "    if ep!=epochs-1:\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            starting_forget_acc = accuracy(net, forget_loader)\n",
    "\n",
    "        if starting_forget_acc > 0.86:\n",
    "            net, counter = forget_step(net, forget_loader, starting_forget_acc, counter_bool)\n",
    "            if counter>=10:\n",
    "                counter_bool+=1\n",
    "                print(f'COUNTER BOOL ON = {counter_bool}')\n",
    "        else:\n",
    "            print('Not doing forget step')\n",
    "\n",
    "    print('\"\"\" \"\"\"'*5)\n",
    "    print('Retrain')\n",
    "    net = retrain_step(net, retain_loader)\n",
    "\n",
    "    ft_forget_losses = compute_losses(net, forget_loader)\n",
    "\n",
    "    ft_mia_scores = calc_mia_acc(ft_forget_losses, val_losses)\n",
    "\n",
    "    print(\n",
    "        f\"The MIA has an accuracy of {ft_mia_scores.mean():.3f} on forgotten vs unseen images\"\n",
    "    )\n",
    "\n",
    "    if np.abs(0.5-ft_mia_scores.mean())<0.01:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.Figure(figsize=(16, 6))\n",
    "\n",
    "plt.title(\n",
    "    f\"Unlearned by fine-tuning.\\nAttack accuracy: {ft_mia_scores.mean():0.2f}\"\n",
    ")\n",
    "plt.hist(val_losses, density=True, alpha=0.5, bins=50, label=\"Test set\")\n",
    "plt.hist(ft_forget_losses, density=True, alpha=0.5, bins=50, label=\"Forget set\")\n",
    "\n",
    "plt.xlabel(\"Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlim((0, np.max(val_losses)))\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
